

<!DOCTYPE html>
<html lang="zh-CN" data-default-color-scheme=auto>



<head>
  <meta charset="UTF-8">
  <link rel="apple-touch-icon" sizes="76x76" href="/img/fluid.png">
  <link rel="icon" href="/img/fluid.png">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=5.0, shrink-to-fit=no">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  
  <meta name="theme-color" content="#2f4154">
  <meta name="author" content="where">
  <meta name="keywords" content="">
  
    <meta name="description" content="凤凰架构微服务九个特征文中列举了微服务的九个核心的业务与技术特征，下面将其一一列出并解读。  围绕业务能力构建（Organized around Business Capability）。这里再次强调了康威定律的重要性，有怎样结构、规模、能力的团队，就会产生出对应结构、规模、能力的产品。这个结论不是某个团队、某个公司遇到的巧合，而是必然的演化结果。如果本应该归属同一个产品内的功能被划分在不同团队中">
<meta property="og:type" content="article">
<meta property="og:title" content="凤凰架构">
<meta property="og:url" content="http://example.com/2023/06/01/%E4%B9%A6%E7%B1%8D-%E5%AD%A6%E4%B9%A0/%E5%87%A4%E5%87%B0%E6%9E%B6%E6%9E%84/index.html">
<meta property="og:site_name" content="where&#39;s blog">
<meta property="og:description" content="凤凰架构微服务九个特征文中列举了微服务的九个核心的业务与技术特征，下面将其一一列出并解读。  围绕业务能力构建（Organized around Business Capability）。这里再次强调了康威定律的重要性，有怎样结构、规模、能力的团队，就会产生出对应结构、规模、能力的产品。这个结论不是某个团队、某个公司遇到的巧合，而是必然的演化结果。如果本应该归属同一个产品内的功能被划分在不同团队中">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://raw.githubusercontent.com/wherezy1/PicGOImageShack/main/images/img.png?token=AUR75H4X3V3ZO7XR27ERTNLEPX2JY">
<meta property="og:image" content="https://jingyecn.top:18080/assets/img/mtl.7974eb9f.png">
<meta property="og:image" content="https://raw.githubusercontent.com/wherezy1/PicGOImageShack/main/images/ixxfdsfafmg.png">
<meta property="og:image" content="https://raw.githubusercontent.com/wherezy1/PicGOImageShack/main/images/%E4%B8%8B%E8%BD%BD.png?token=AUR75H2X3PHGB5QTDO72IZTEQU5MO">
<meta property="og:image" content="https://jingyecn.top:18080/assets/img/pv-pvc.74b08dd6.png">
<meta property="og:image" content="https://jingyecn.top:18080/assets/img/storage-class.4d160b17.png">
<meta property="og:image" content="https://raw.githubusercontent.com/wherezy1/PicGOImageShack/main/images/%E4%B8%8B%E8%BD%BD%20(1).png?token=AUR75HYFUW7PUZJREHST5P3EQXPEQ">
<meta property="og:image" content="https://raw.githubusercontent.com/wherezy1/PicGOImageShack/main/images/context.758f5879.png?token=AUR75H23ATNE5HXHXVD5ZIDEQXPA2">
<meta property="og:image" content="https://raw.githubusercontent.com/wherezy1/PicGOImageShack/main/images/image-20230611230524846.png?token=AUR75H3OXGJSISTN4Z4NF7TEQXRPC">
<meta property="og:image" content="https://raw.githubusercontent.com/wherezy1/PicGOImageShack/main/images/image-20230611230516301.png?token=AUR75H3DUQ26IVG4IDVU3XDEQXROS">
<meta property="og:image" content="https://jingyecn.top:18080/assets/img/service-mesh-3.b272ad56.png">
<meta property="og:image" content="https://jingyecn.top:18080/assets/img/service-mesh-4.ad21d5d1.png">
<meta property="og:image" content="https://jingyecn.top:18080/assets/img/service-mesh-5.71d1ba42.png">
<meta property="og:image" content="https://jingyecn.top:18080/assets/img/istio-arch.9a97cf23.png">
<meta property="article:published_time" content="2023-05-31T16:00:00.000Z">
<meta property="article:modified_time" content="2023-06-22T15:33:40.709Z">
<meta property="article:author" content="where">
<meta property="article:tag" content="书籍">
<meta property="article:tag" content="笔记">
<meta name="twitter:card" content="summary_large_image">
<meta name="twitter:image" content="https://raw.githubusercontent.com/wherezy1/PicGOImageShack/main/images/img.png?token=AUR75H4X3V3ZO7XR27ERTNLEPX2JY">
  
  
  
  <title>凤凰架构 - where&#39;s blog</title>

  <link  rel="stylesheet" href="https://lib.baomitu.com/twitter-bootstrap/4.6.1/css/bootstrap.min.css" />



  <link  rel="stylesheet" href="https://lib.baomitu.com/github-markdown-css/4.0.0/github-markdown.min.css" />

  <link  rel="stylesheet" href="https://lib.baomitu.com/hint.css/2.7.0/hint.min.css" />

  <link  rel="stylesheet" href="https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.css" />



<!-- 主题依赖的图标库，不要自行修改 -->
<!-- Do not modify the link that theme dependent icons -->

<link rel="stylesheet" href="//at.alicdn.com/t/font_1749284_hj8rtnfg7um.css">



<link rel="stylesheet" href="//at.alicdn.com/t/font_1736178_lbnruvf0jn.css">


<link  rel="stylesheet" href="/css/main.css" />


  <link id="highlight-css" rel="stylesheet" href="/css/highlight.css" />
  
    <link id="highlight-css-dark" rel="stylesheet" href="/css/highlight-dark.css" />
  




  <script id="fluid-configs">
    var Fluid = window.Fluid || {};
    Fluid.ctx = Object.assign({}, Fluid.ctx)
    var CONFIG = {"hostname":"example.com","root":"/","version":"1.9.4","typing":{"enable":true,"typeSpeed":70,"cursorChar":"_","loop":false,"scope":[]},"anchorjs":{"enable":true,"element":"h1,h2,h3,h4,h5,h6","placement":"left","visible":"hover","icon":""},"progressbar":{"enable":true,"height_px":3,"color":"#29d","options":{"showSpinner":false,"trickleSpeed":100}},"code_language":{"enable":true,"default":"TEXT"},"copy_btn":true,"image_caption":{"enable":true},"image_zoom":{"enable":true,"img_url_replace":["",""]},"toc":{"enable":true,"placement":"right","headingSelector":"h1,h2,h3,h4,h5,h6","collapseDepth":0},"lazyload":{"enable":true,"loading_img":"/img/loading.gif","onlypost":false,"offset_factor":2},"web_analytics":{"enable":false,"follow_dnt":true,"baidu":null,"google":null,"gtag":null,"tencent":{"sid":null,"cid":null},"woyaola":null,"cnzz":null,"leancloud":{"app_id":null,"app_key":null,"server_url":null,"path":"window.location.pathname","ignore_local":false}},"search_path":"/local-search.xml"};

    if (CONFIG.web_analytics.follow_dnt) {
      var dntVal = navigator.doNotTrack || window.doNotTrack || navigator.msDoNotTrack;
      Fluid.ctx.dnt = dntVal && (dntVal.startsWith('1') || dntVal.startsWith('yes') || dntVal.startsWith('on'));
    }
  </script>
  <script  src="/js/utils.js" ></script>
  <script  src="/js/color-schema.js" ></script>
  


  
<meta name="generator" content="Hexo 6.3.0"></head>


<body>
  

  <header>
    

<div class="header-inner" style="height: 70vh;">
  <nav id="navbar" class="navbar fixed-top  navbar-expand-lg navbar-dark scrolling-navbar">
  <div class="container">
    <a class="navbar-brand" href="/">
      <strong>Fluid</strong>
    </a>

    <button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse"
            data-target="#navbarSupportedContent"
            aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
      <div class="animated-icon"><span></span><span></span><span></span></div>
    </button>

    <!-- Collapsible content -->
    <div class="collapse navbar-collapse" id="navbarSupportedContent">
      <ul class="navbar-nav ml-auto text-center">
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/">
                <i class="iconfont icon-home-fill"></i>
                <span>首页</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/archives/">
                <i class="iconfont icon-archive-fill"></i>
                <span>归档</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/categories/">
                <i class="iconfont icon-category-fill"></i>
                <span>分类</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/tags/">
                <i class="iconfont icon-tags-fill"></i>
                <span>标签</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/about/">
                <i class="iconfont icon-user-fill"></i>
                <span>关于</span>
              </a>
            </li>
          
        
        
          <li class="nav-item" id="search-btn">
            <a class="nav-link" target="_self" href="javascript:;" data-toggle="modal" data-target="#modalSearch" aria-label="Search">
              <i class="iconfont icon-search"></i>
            </a>
          </li>
          
        
        
          <li class="nav-item" id="color-toggle-btn">
            <a class="nav-link" target="_self" href="javascript:;" aria-label="Color Toggle">
              <i class="iconfont icon-dark" id="color-toggle-icon"></i>
            </a>
          </li>
        
      </ul>
    </div>
  </div>
</nav>

  

<div id="banner" class="banner" parallax=true
     style="background: url('/img/default.png') no-repeat center center; background-size: cover;">
  <div class="full-bg-img">
    <div class="mask flex-center" style="background-color: rgba(0, 0, 0, 0.3)">
      <div class="banner-text text-center fade-in-up">
        <div class="h2">
          
            <span id="subtitle" data-typed-text="凤凰架构"></span>
          
        </div>

        
          
  <div class="mt-3">
    
    
      <span class="post-meta">
        <i class="iconfont icon-date-fill" aria-hidden="true"></i>
        <time datetime="2023-06-01 00:00" pubdate>
          2023年6月1日 凌晨
        </time>
      </span>
    
  </div>

  <div class="mt-1">
    
      <span class="post-meta mr-2">
        <i class="iconfont icon-chart"></i>
        
          35k 字
        
      </span>
    

    
      <span class="post-meta mr-2">
        <i class="iconfont icon-clock-fill"></i>
        
        
        
          290 分钟
        
      </span>
    

    
    
  </div>


        
      </div>

      
    </div>
  </div>
</div>

</div>

  </header>

  <main>
    
      

<div class="container-fluid nopadding-x">
  <div class="row nomargin-x">
    <div class="side-col d-none d-lg-block col-lg-2">
      

    </div>

    <div class="col-lg-8 nopadding-x-md">
      <div class="container nopadding-x-md" id="board-ctn">
        <div id="board">
          <article class="post-content mx-auto">
            <!-- SEO header -->
            <h1 style="display: none">凤凰架构</h1>
            
            
              <div class="markdown-body">
                
                <h1 id="凤凰架构"><a href="#凤凰架构" class="headerlink" title="凤凰架构"></a>凤凰架构</h1><h2 id="微服务九个特征"><a href="#微服务九个特征" class="headerlink" title="微服务九个特征"></a>微服务九个特征</h2><p>文中列举了微服务的九个核心的业务与技术特征，下面将其一一列出并解读。</p>
<ul>
<li><strong>围绕业务能力构建</strong>（Organized around Business Capability）。这里再次强调了康威定律的重要性，有怎样结构、规模、能力的团队，就会产生出对应结构、规模、能力的产品。这个结论不是某个团队、某个公司遇到的巧合，而是必然的演化结果。如果本应该归属同一个产品内的功能被划分在不同团队中，必然会产生大量的跨团队沟通协作，跨越团队边界无论在管理、沟通、工作安排上都有更高昂的成本，高效的团队自然会针对其进行改进，当团队、产品磨合调节稳定之后，团队与产品就会拥有一致的结构。</li>
<li><strong>分散治理</strong>（Decentralized Governance）。这是要表达“谁家孩子谁来管”的意思，服务对应的开发团队有直接对服务运行质量负责的责任，也应该有着不受外界干预地掌控服务各个方面的权力，譬如选择与其他服务异构的技术来实现自己的服务。这一点在真正实践时多少存有宽松的处理余地，大多数公司都不会在某一个服务使用 Java，另一个用 Python，下一个用 Golang，而是通常会有统一的主流语言，乃至统一的技术栈或专有的技术平台。微服务不提倡也并不反对这种“统一”，只要负责提供和维护基础技术栈的团队，有被各方依赖的觉悟，要有“经常被凌晨 3 点的闹钟吵醒”的心理准备就好。微服务更加强调的是确实有必要技术异构时，应能够有选择“不统一”的权利，譬如不应该强迫 Node.js 去开发报表页面，要做人工智能训练模型时，应该可以选择 Python，等等。</li>
<li><strong>通过服务来实现独立自治的组件</strong>（Componentization via Services）。之所以强调通过“服务”（Service）而不是“类库”（Library）来构建组件，是因为类库在编译期静态链接到程序中，通过本地调用来提供功能，而服务是进程外组件，通过远程调用来提供功能。前面的文章里我们已经分析过，尽管远程服务有更高昂的调用成本，但这是为组件带来隔离与自治能力的必要代价。</li>
<li><strong>产品化思维</strong>（Products not Projects）。避免把软件研发视作要去完成某种功能，而是视作一种持续改进、提升的过程。譬如，不应该把运维只看作运维团队的事，把开发只看作开发团队的事，团队应该为软件产品的整个生命周期负责，开发者不仅应该知道软件如何开发，还应该知道它如何运作，用户如何反馈，乃至售后支持工作是怎样进行的。注意，这里服务的用户不一定是最终用户，也可能是消费这个服务的另外一个服务。以前在单体架构下，程序的规模决定了无法让全部人员都关注完整的产品，组织中会有开发、运维、支持等细致的分工的成员，各人只关注于自己的一块工作，但在微服务下，要求开发团队中每个人都具有产品化思维，关心整个产品的全部方面是具有可行性的。</li>
<li><strong>数据去中心化</strong>（Decentralized Data Management）。微服务明确地提倡数据应该按领域分散管理、更新、维护、存储，在单体服务中，一个系统的各个功能模块通常会使用同一个数据库，诚然中心化的存储天生就更容易避免一致性问题，但是，同一个数据实体在不同服务的视角里，它的抽象形态往往也是不同的。譬如，Bookstore 应用中的书本，在销售领域中关注的是价格，在仓储领域中关注的库存数量，在商品展示领域中关注的是书籍的介绍信息，如果作为中心化的存储，所有领域都必须修改和映射到同一个实体之中，这便使得不同的服务很可能会互相产生影响而丧失掉独立性。尽管在分布式中要处理好一致性的问题也相当困难，很多时候都没法使用传统的事务处理来保证，但是两害相权取其轻，有一些必要的代价仍是值得付出的。</li>
<li><strong>强终端弱管道</strong>（Smart Endpoint and Dumb Pipe）。弱管道（Dumb Pipe）几乎算是直接指名道姓地反对 SOAP 和 ESB 的那一堆复杂的通信机制。ESB 可以处理消息的编码加工、业务规则转换等；BPM 可以集中编排企业业务服务；SOAP 有几十个 WS-*协议族在处理事务、一致性、认证授权等一系列工作，这些构筑在通信管道上的功能也许对某个系统中的某一部分服务是有必要的，但对于另外更多的服务则是强加进来的负担。如果服务需要上面的额外通信能力，就应该在服务自己的 Endpoint 上解决，而不是在通信管道上一揽子处理。微服务提倡类似于经典 UNIX 过滤器那样简单直接的通信方式，RESTful 风格的通信在微服务中会是更加合适的选择。</li>
<li><strong>容错性设计</strong>（Design for Failure）。不再虚幻地追求服务永远稳定，而是接受服务总会出错的现实，要求在微服务的设计中，有自动的机制对其依赖的服务能够进行快速故障检测，在持续出错的时候进行隔离，在服务恢复的时候重新联通。所以“断路器”这类设施，对实际生产环境的微服务来说并不是可选的外围组件，而是一个必须的支撑点，如果没有容错性的设计，系统很容易就会被因为一两个服务的崩溃所带来的雪崩效应淹没。可靠系统完全可能由会出错的服务组成，这是微服务最大的价值所在，也是这部开源文档标题“凤凰架构”的含义。</li>
<li><strong>演进式设计</strong>（Evolutionary Design）。容错性设计承认服务会出错，演进式设计则是承认服务会被报废淘汰。一个设计良好的服务，应该是能够报废的，而不是期望得到长存永生。假如系统中出现不可更改、无可替代的服务，这并不能说明这个服务是多么的优秀、多么的重要，反而是一种系统设计上脆弱的表现，微服务所追求的独立、自治，也是反对这种脆弱性的表现。</li>
<li><strong>基础设施自动化</strong>（Infrastructure Automation）。基础设施自动化，如 CI&#x2F;CD 的长足发展，显著减少了构建、发布、运维工作的复杂性。由于微服务下运维的对象比起单体架构要有数量级的增长，使用微服务的团队更加依赖于基础设施的自动化，人工是很难支撑成百上千乃至成千上万级别的服务的。</li>
</ul>
<p>《Microservices》一文中对微服务特征的描写已经相当具体了，文中除了定义微服务是什么，还专门申明了微服务不是什么——微服务不是 SOA 的变体或衍生品，应该明确地与 SOA 划清了界线，不再贴上任何 SOA 的标签。如此，微服务的概念才算是一种真正丰满、独立的架构风格，为它在未来的几年时间里如明星一般闪耀崛起于技术舞台铺下了理论基础。</p>
<h1 id="分布式的基石"><a href="#分布式的基石" class="headerlink" title="- 分布式的基石"></a>- 分布式的基石</h1><h2 id="服务发现"><a href="#服务发现" class="headerlink" title="服务发现"></a>服务发现</h2><p>至少有（但不限于）以下三个问题是必须考虑并得到妥善解决的：</p>
<p>对消费者来说，外部的服务由谁提供？具体在什么网络位置？<br>对生产者来说，内部哪些服务需要暴露？哪些应当隐藏？应当以何种形式暴露服务？以什么规则在集群中分配请求？<br>对调用过程来说，如何保证每个远程服务都接收到相对平均的流量，获得尽可能高的服务质量与可靠性？<br>这三个问题的解决方案，在微服务架构中通常被称为“服务发现”、“服务的网关路由”和“服务的负载均衡”。</p>
<h1 id="网关"><a href="#网关" class="headerlink" title="网关"></a>网关</h1><h2 id="BBF网关"><a href="#BBF网关" class="headerlink" title="BBF网关"></a>BBF网关</h2><p>提到网关的唯一性、高可用与扩展，笔者顺带也说一下近年来随着微服务一起火起来的概念“BFF”（Backends for Frontends）。这个概念目前还没有权威的中文翻译，在我们讨论的上下文里，它的意思是，网关不必为所有的前端提供无差别的服务，而是应该针对不同的前端，聚合不同的服务，提供不同的接口和网络访问协议支持。譬如，运行于浏览器的 Web 程序，由于浏览器一般只支持 HTTP 协议，服务网关就应提供 REST 等基于 HTTP 协议的服务，但同时我们亦可以针对运行于桌面系统的程序部署另外一套网关，它能与 Web 网关有完全不同的技术选型，能提供出基于更高性能协议（如 gRPC）的接口来获得更好的体验。在网关这种边缘节点上，针对同一样的后端集群，裁剪、适配、聚合出适应不一样的前端的服务，有助于后端的稳定，也有助于前端的赋能。</p>
<p><img src="https://raw.githubusercontent.com/wherezy1/PicGOImageShack/main/images/img.png?token=AUR75H4X3V3ZO7XR27ERTNLEPX2JY" srcset="/img/loading.gif" lazyload alt="img">g)</p>
<p>图 7-3 BFF 网关</p>
<h1 id="流量治理"><a href="#流量治理" class="headerlink" title="流量治理"></a>流量治理</h1><h2 id="流量容错"><a href="#流量容错" class="headerlink" title="流量容错"></a>流量容错</h2><p>熔断与降级</p>
<figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs">服务熔断和服务降级之间的联系与差别。断路器做的事情是自动进行服务熔断，这是一种快速失败的容错策略的实现方法。在快速失败策略明确反馈了故障信息给上游服务以后，上游服务必须能够主动处理调用失败的后果，而不是坐视故障扩散，这里的“处理”指的就是一种典型的服务降级逻辑，降级逻辑可以包括，但不应该仅仅限于是把异常信息抛到用户界面去，而应该尽力想办法通过其他路径解决问题，譬如把原本要处理的业务记录下来，留待以后重新处理是最低限度的通用降级逻辑。<br></code></pre></td></tr></table></figure>

<h3 id="容错策略"><a href="#容错策略" class="headerlink" title="容错策略"></a>容错策略</h3><p>表 8-1 常见容错策略优缺点及应用场景对比</p>
<table>
<thead>
<tr>
<th>容错策略</th>
<th>优点</th>
<th>缺点</th>
<th>应用场景</th>
</tr>
</thead>
<tbody><tr>
<td><strong>故障转移</strong></td>
<td>系统自动处理，调用者对失败的信息不可见</td>
<td>增加调用时间，额外的资源开销</td>
<td>调用幂等服务 对调用时间不敏感的场景</td>
</tr>
<tr>
<td><strong>快速失败</strong></td>
<td>调用者有对失败的处理完全控制权 不依赖服务的幂等性</td>
<td>调用者必须正确处理失败逻辑，如果一味只是对外抛异常，容易引起雪崩</td>
<td>调用非幂等的服务 超时阈值较低的场景</td>
</tr>
<tr>
<td><strong>安全失败</strong></td>
<td>不影响主路逻辑</td>
<td>只适用于旁路调用</td>
<td>调用链中的旁路服务</td>
</tr>
<tr>
<td><strong>沉默失败</strong></td>
<td>控制错误不影响全局</td>
<td>出错的地方将在一段时间内不可用</td>
<td>频繁超时的服务</td>
</tr>
<tr>
<td><strong>故障恢复</strong></td>
<td>调用失败后自动重试，也不影响主路逻辑</td>
<td>重试任务可能产生堆积，重试仍然可能失败</td>
<td>调用链中的旁路服务 对实时性要求不高的主路逻辑也可以使用</td>
</tr>
<tr>
<td><strong>并行调用</strong></td>
<td>尽可能在最短时间内获得最高的成功率</td>
<td>额外消耗机器资源，大部分调用可能都是无用功</td>
<td>资源充足且对失败容忍度低的场景</td>
</tr>
<tr>
<td><strong>广播调用</strong></td>
<td>支持同时对批量的服务提供者发起调用</td>
<td>资源消耗大，失败概率高</td>
<td>只适用于批量操作的场景</td>
</tr>
</tbody></table>
<h3 id="容错设计模式"><a href="#容错设计模式" class="headerlink" title="容错设计模式"></a>容错设计模式</h3><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs">用断路器模式实现快速失败策略，使用舱壁隔离模式实现静默失败策略，在断路器中举例的主动对非关键的旁路服务进行降级，亦可算作是对安全失败策略的一种体现。那还剩下故障转移和故障恢复两种策略的实现尚未涉及。接下来，笔者以重试模式来介绍这两种容错策略的主流实现方案。<br></code></pre></td></tr></table></figure>

<p>断路器模式</p>
<p>舱壁隔离模式</p>
<p>重试模式</p>
<h2 id="限流控制"><a href="#限流控制" class="headerlink" title="限流控制"></a>限流控制</h2><h3 id="流量统计指标"><a href="#流量统计指标" class="headerlink" title="流量统计指标"></a>流量统计指标</h3><p>要做流量控制，首先要弄清楚到底哪些指标能反映系统的流量压力大小。相较而言，容错的统计指标是明确的，容错的触发条件基本上只取决于请求的故障率，发生失败、拒绝与超时都算作故障；但限流的统计指标就不那么明确了，限流中的“流”到底指什么呢？要解答这个问题，我们先来理清经常用于衡量服务流量压力，但又较容易混淆的三个指标的定义：</p>
<ul>
<li><strong>每秒事务数</strong>（Transactions per Second，TPS）：TPS 是衡量信息系统吞吐量的最终标准。“事务”可以理解为一个逻辑上具备原子性的业务操作。譬如你在 Fenix’s Bookstore 买了一本书，将要进行支付，“支付”就是一笔业务操作，支付无论成功还是不成功，这个操作在逻辑上是原子的，即逻辑上不可能让你买本书还成功支付了前面 200 页，又失败了后面 300 页。</li>
<li><strong>每秒请求数</strong>（Hits per Second，HPS）：HPS 是指每秒从客户端发向服务端的请求数（请将 Hits 理解为 Requests 而不是 Clicks，国内某些翻译把它理解为“每秒点击数”多少有点望文生义的嫌疑）。如果只要一个请求就能完成一笔业务，那 HPS 与 TPS 是等价的，但在一些场景（尤其常见于网页中）里，一笔业务可能需要多次请求才能完成。譬如你在 Fenix’s Bookstore 买了一本书要进行支付，尽管逻辑上它是原子的，但技术实现上，除非你是直接在银行开的商城中购物能够直接扣款，否则这个操作就很难在一次请求里完成，总要经过显示支付二维码、扫码付款、校验支付是否成功等过程，中间不可避免地会发生多次请求。</li>
<li><strong>每秒查询数</strong>（Queries per Second，QPS）：QPS 是指一台服务器能够响应的查询次数。如果只有一台服务器来应答请求，那 QPS 和 HPS 是等价的，但在分布式系统中，一个请求的响应往往要由后台多个服务节点共同协作来完成。譬如你在 Fenix’s Bookstore 买了一本书要进行支付，尽管扫描支付二维码时客户端只发送了一个请求，但这背后服务端很可能需要向仓储服务确认库存信息避免超卖、向支付服务发送指令划转货款、向用户服务修改用户的购物积分，等等，这里面每次内部访问都要消耗掉一次或多次查询数。</li>
</ul>
<h3 id="单机限流"><a href="#单机限流" class="headerlink" title="单机限流"></a>单机限流</h3><p>固定窗口 &#x2F; 计数器算法</p>
<p>滑动窗口算法</p>
<p>漏桶算法</p>
<p>令牌桶算法</p>
<figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs markdown">常用的分布式限流算法包括：令牌桶算法、漏桶算法、计数器算法、滑动窗口算法以及基于Bloom Filter的算法。<br><br><span class="hljs-bullet">1.</span> 令牌桶算法：令牌桶算法是一种基于令牌的限流算法，以固定的速率往桶中放入令牌，请求需要取出令牌才能执行。若桶中没有足够的令牌，请求将会被暂时阻塞。时间突发性的流量可以通过令牌桶算法平滑处理，速率可通过调整令牌放入的速度来调节。优点是可以平滑处理流量，弱化了突发流量，缺点是对于大流量的处理性能开销比较大。<br><br><span class="hljs-bullet">2.</span> 漏桶算法：漏桶算法是一种广泛应用的简单的限流算法，从一个固定大小的桶中以匀速的速度发送请求（或从桶中漏出）直到达到最大容量。当桶满了之后，请求将被拒绝。漏桶算法对于突发流量的处理能力较弱，但是可以对请求进行严格的限制，防止系统产生超负荷，可以避免因为恶意请求威胁业务的稳定性。<br><br><span class="hljs-bullet">3.</span> 计数器算法：计数器算法基于计数器对通过的请求进行计数，达到一定的阈值之后拒绝请求。优点是简单易懂，实现也比较简单，但是不适用于突发流量，很容易被恶意攻击规避。<br><br><span class="hljs-bullet">4.</span> 滑动窗口算法：滑动窗口算法是根据某个固定时间窗口内的请求数量来限制流量。当某个时间窗口内的请求数量超出阈值后，后继请求将被限制。滑动窗口算法较为灵活，能够根据系统容量来适时调整时间窗口的大小，支持比较灵活的规则配置。<br><br><span class="hljs-bullet">5.</span> 基于Bloom Filter的算法：基于Bloom Filter的算法通过将判断请求是否在布隆过滤器中存在来实现限流，具有内存占用小、吞吐量大、误差较小的优点。但是由于误判问题的存在，可能会将一部分正常的请求误判为恶意请求。<br><br>总体而言，选择哪种分布式限流算法应根据具体业务场景和实际需求，综合考虑算法的适用性、性能、开销和误差等因素<br></code></pre></td></tr></table></figure>



<h3 id="分布式限流"><a href="#分布式限流" class="headerlink" title="分布式限流"></a>分布式限流</h3><h1 id="可观测性"><a href="#可观测性" class="headerlink" title="可观测性"></a>可观测性</h1><p>日志、追踪、度量</p>
<p><img src="https://jingyecn.top:18080/assets/img/mtl.7974eb9f.png" srcset="/img/loading.gif" lazyload alt="img"></p>
<ul>
<li><strong>日志</strong>（Logging）：日志的职责是记录离散事件，通过这些记录事后分析出程序的行为，譬如曾经调用过什么方法，曾经操作过哪些数据，等等。打印日志被认为是程序中最简单的工作之一，调试问题时常有人会说“当初这里记得打点日志就好了”，可见这就是一项举手之劳的任务。输出日志的确很容易，但收集和分析日志却可能会很复杂，面对成千上万的集群节点，面对迅速滚动的事件信息，面对数以 TB 计算的文本，传输与归集都并不简单。对大多数程序员来说，分析日志也许就是最常遇见也最有实践可行性的“大数据系统”了。</li>
<li><strong>追踪</strong>（Tracing）：单体系统时代追踪的范畴基本只局限于<a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Stack_trace">栈追踪 (opens new window)</a>（Stack Tracing），调试程序时，在 IDE 打个断点，看到的 Call Stack 视图上的内容便是追踪；编写代码时，处理异常调用了 <code>Exception::printStackTrace()</code>方法，它输出的堆栈信息也是追踪。微服务时代，追踪就不只局限于调用栈了，一个外部请求需要内部若干服务的联动响应，这时候完整的调用轨迹将跨越多个服务，同时包括服务间的网络传输信息与各个服务内部的调用堆栈信息，因此，分布式系统中的追踪在国内常被称为“全链路追踪”（后文就直接称“链路追踪”了），许多资料中也称它为“<a target="_blank" rel="noopener" href="https://opentracing.io/docs/overview/what-is-tracing/">分布式追踪 (opens new window)</a>”（Distributed Tracing）。追踪的主要目的是排查故障，如分析调用链的哪一部分、哪个方法出现错误或阻塞，输入输出是否符合预期，等等。</li>
<li><strong>度量</strong>（Metrics）：度量是指对系统中某一类信息的统计聚合。譬如，证券市场的每一只股票都会定期公布财务报表，通过财报上的营收、净利、毛利、资产、负债等等一系列数据来体现过去一个财务周期中公司的经营状况，这便是一种信息聚合。Java 天生自带有一种基本的度量，就是由虚拟机直接提供的 JMX（Java Management eXtensions）度量，诸如内存大小、各分代的用量、峰值的线程数、垃圾收集的吞吐量、频率，等等都可以从 JMX 中获得。度量的主要目的是监控（Monitoring）和预警（Alert），如某些度量指标达到风险阈值时触发事件，以便自动处理或者提醒管理员介入。</li>
</ul>
<h2 id="日志"><a href="#日志" class="headerlink" title="日志"></a>日志</h2><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs"><br></code></pre></td></tr></table></figure>



<h2 id="链路追踪"><a href="#链路追踪" class="headerlink" title="链路追踪"></a>链路追踪</h2><h1 id="不可变基础设施"><a href="#不可变基础设施" class="headerlink" title="- 不可变基础设施"></a>- 不可变基础设施</h1><h1 id="–-Kubernetes"><a href="#–-Kubernetes" class="headerlink" title="– Kubernetes"></a>– Kubernetes</h1><h2 id="概念"><a href="#概念" class="headerlink" title="概念"></a>概念</h2><p><img src="https://raw.githubusercontent.com/wherezy1/PicGOImageShack/main/images/ixxfdsfafmg.png" srcset="/img/loading.gif" lazyload alt="img"></p>
<ul>
<li><strong>容器</strong>（Container）：延续了自 Docker 以来一个容器封装一个应用进程的理念，是镜像管理的最小单位。</li>
<li><strong>生产任务</strong>（Pod）：补充了容器化后缺失的与进程组对应的“容器组”的概念，Pod 中容器共享 UTS、IPC、网络等名称空间，是资源调度的最小单位。</li>
<li><strong>节点</strong>（Node）：对应于集群中的单台机器，这里的机器即可以是生产环境中的物理机，也可以是云计算环境中的虚拟节点，节点是处理器和内存等资源的资源池，是硬件单元的最小单位。</li>
<li><strong>集群</strong>（Cluster）：对应于整个集群，Kubernetes 提倡理念是面向集群来管理应用。当你要部署应用的时候，只需要通过声明式 API 将你的意图写成一份元数据（Manifests），将它提交给集群即可，而无需关心它具体分配到哪个节点（尽管通过标签选择器完全可以控制它分配到哪个节点，但一般不需要这样做）、如何实现 Pod 间通信、如何保证韧性与弹性，等等，所以集群是处理元数据的最小单位。</li>
<li><strong>集群联邦</strong>（Federation）：对应于多个集群，通过联邦可以统一管理多个 Kubernetes 集群，联邦的一种常见应用是支持跨可用区域多活、跨地域容灾的需求。</li>
</ul>
<h2 id="资源-与-控制器"><a href="#资源-与-控制器" class="headerlink" title="资源 与 控制器"></a>资源 与 控制器</h2><blockquote>
<p>额外知识：Kubernates 的资源对象与控制器</p>
<p>目前，Kubernetes 已内置支持相当多的资源对象，并且还可以使用<a target="_blank" rel="noopener" href="https://jingyecn.top:18080/immutable-infrastructure/extension/crd.html">CRD</a>（Custom Resource Definition）来自定义扩充，你可以使用<code>kubectl api-resources</code>来查看它们。笔者根据用途分类列举了以下常见的资源：</p>
<ul>
<li>用于描述如何创建、销毁、更新、扩缩 Pod，包括：Autoscaling（HPA）、CronJob、DaemonSet、Deployment、Job、Pod、ReplicaSet、StatefulSet</li>
<li>用于配置信息的设置与更新，包括：ConfigMap、Secret</li>
<li>用于持久性地存储文件或者 Pod 之间的文件共享，包括：Volume、LocalVolume、PersistentVolume、PersistentVolumeClaim、StorageClass</li>
<li>用于维护网络通信和服务访问的安全，包括：SecurityContext、ServiceAccount、Endpoint、NetworkPolicy</li>
<li>用于定义服务与访问，包括：Ingress、Service、EndpointSlice</li>
<li>用于划分虚拟集群、节点和资源配额，包括：Namespace、Node、ResourceQuota</li>
</ul>
<p>这些资源在控制器管理框架中一般都会有相应的控制器来管理，笔者列举常见的控制器，按照它们的启动情况分类如下：</p>
<ul>
<li>必须启用的控制器：EndpointController、ReplicationController、PodGCController、ResourceQuotaController、NamespaceController、ServiceAccountController、GarbageCollectorController、DaemonSetController、JobController、DeploymentController、ReplicaSetController、HPAController、DisruptionController、StatefulSetController、CronJobController、CSRSigningController、CSRApprovingController、TTLController</li>
<li>默认启用的可选控制器，可通过选项禁止：TokenController、NodeController、ServiceController、RouteController、PVBinderController、AttachDetachController</li>
<li>默认禁止的可选控制器，可通过选项启用：BootstrapSignerController、TokenCleanerController</li>
</ul>
</blockquote>
<h2 id="弹性-与-韧性"><a href="#弹性-与-韧性" class="headerlink" title="弹性 与 韧性"></a>弹性 与 韧性</h2><p>故障恢复、滚动更新、自动扩缩这些特性，在云原生中时代里常被概括成服务的弹性（Elasticity）与韧性（Resilience），ReplicaSet、Deployment、Autoscaling 的用法，也属于是所有 Kubernetes 教材资料都会讲到的“基础必修课”。如果你准备学习 Kubernetes 或者其他云原生相关技术，笔者建议最好不要死记硬背地学习每个资源的元数据文件该如何编写、有哪些指令、有哪些功能，更好的方式是站在解决问题的角度去理解为什么 Kubernetes 要设计这些资源和控制器，理解为什么这些资源和控制器会被设计成现在这种样子。</p>
<h2 id="零信任安全模型的特征"><a href="#零信任安全模型的特征" class="headerlink" title="零信任安全模型的特征"></a>零信任安全模型的特征</h2><p>Google论文地址：《<a target="_blank" rel="noopener" href="https://cloud.google.com/security/beyondprod">BeyondProd: A New Approach to Cloud-Native Security (opens new window)</a>》</p>
<p>零信任安全的中心思想是不应当以某种固有特征来自动信任任何流量，除非明确得到了能代表请求来源（不一定是人，更可能是另一台服务）的身份凭证，否则一律不会有默认的信任关系。在 2019 年，Google 发表了一篇在安全与研发领域里都备受关注的论文《<a target="_blank" rel="noopener" href="https://cloud.google.com/security/beyondprod">BeyondProd: A New Approach to Cloud-Native Security (opens new window)</a>》（BeyondCorp 和 BeyondProd 是谷歌最新一代安全框架的名字，从 2014 年起已连续发表了 6 篇关于 BeyondCorp 和 BeyondProd 的论文），此文中详细列举了传统的基于边界的网络安全模型与云原生时代下基于零信任网络的安全模型之间的差异，并描述了要完成边界安全模型到零信任安全模型的迁移所要实现的具体需求点，笔者将其翻译转述为如表 9-1 所示内容。</p>
<p>表 9-1 传统网络安全模型与云原生时代零信任模型对比</p>
<table>
<thead>
<tr>
<th align="left">传统、边界安全模型</th>
<th align="left">云原生、零信任安全模型</th>
<th align="left">具体需求</th>
</tr>
</thead>
<tbody><tr>
<td align="left">基于防火墙等设施，认为边界内可信</td>
<td align="left">服务到服务通信需认证，环境内的服务之间默认没有信任</td>
<td align="left">保护网络边界（仍然有效）；服务之间默认没有互信</td>
</tr>
<tr>
<td align="left">用于特定的 IP 和硬件（机器）</td>
<td align="left">资源利用率、重用、共享更好，包括 IP 和硬件</td>
<td align="left">受信任的机器运行来源已知的代码</td>
</tr>
<tr>
<td align="left">基于 IP 的身份</td>
<td align="left">基于服务的身份</td>
<td align="left">同上</td>
</tr>
<tr>
<td align="left">服务运行在已知的、可预期的服务器上</td>
<td align="left">服务可运行在环境中的任何地方，包括私有云&#x2F;公有云混合部署</td>
<td align="left">同上</td>
</tr>
<tr>
<td align="left">安全相关的需求由应用来实现，每个应用单独实现</td>
<td align="left">由基础设施来实现，基础设施中集成了共享的安全性要求。</td>
<td align="left">集中策略实施点（Choke Points），一致地应用到所有服务</td>
</tr>
<tr>
<td align="left">对服务如何构建、评审、实施的安全需求的约束力较弱</td>
<td align="left">安全相关的需求一致地应用到所有服务</td>
<td align="left">同上</td>
</tr>
<tr>
<td align="left">安全组件的可观测性较弱</td>
<td align="left">有安全策略及其是否生效的全局视图</td>
<td align="left">同上</td>
</tr>
<tr>
<td align="left">发布不标准，发布频率较低</td>
<td align="left">标准化的构建和发布流程，每个微服务变更独立，变更更频繁</td>
<td align="left">简单、自动、标准化的变更发布流程</td>
</tr>
<tr>
<td align="left">工作负载通常作为虚拟机部署或部署到物理主机，并使用物理机或管理程序进行隔离</td>
<td align="left">封装的工作负载及其进程在共享的操作系统中运行，并有管理平台提供的某种机制来进行隔离</td>
<td align="left">在共享的操作系统的工作负载之间进行隔离</td>
</tr>
</tbody></table>
<h2 id="Google-的实践探索"><a href="#Google-的实践探索" class="headerlink" title="Google 的实践探索"></a>Google 的实践探索</h2><p>笔者照 Google 论文所述来回答这个问题：为了保护服务集群内的代码与基础设施，Google 设计了一系列的内部工具，才最终得以实现前面所说的那些安全原则：</p>
<ul>
<li>为了在网络边界上保护内部服务免受 DDoS 攻击，设计了名为 Google Front End（名字意为“最终用户访问请求的终点”）的边缘代理，负责保证此后所有流量都在 TLS 之上传输，并自动将流量路由到适合的可用区域之中。</li>
<li>为了强制身份只来源于服务，设计了名为 Application Layer Transport Security（应用层传输安全）的服务认证机制，这是一个用于双向认证和传输加密的系统，自动将服务与它的身份标识符绑定，使得所有服务间流量都不必再使用服务名称、主机 IP 来判断对方的身份。</li>
<li>为了确保服务间不再有默认的信任关系，设计了 Service Access Policy（服务访问策略）来管理一个服务向另一个服务发起请求时所需提供的认证、鉴权和审计策略，并支持全局视角的访问控制与分析，以达成“集中、共享的安全策略实施点”这条原则。</li>
<li>为了实现仅以受信的机器运行来源已知的代码，设计了名为 Binary Authorization（二进制授权）的部署时检查机制，确保在软件供应链的每一个阶段，都符合内部安全检查策略，并对此进行授权与鉴权。同时设计了名为 Host Integrity（宿主机完整性）的机器安全启动程序，在创建宿主机时自动验证包括 BIOS、BMC、Bootloader 和操作系统内核的数字签名。</li>
<li>为了工作负载能够具有强隔离性，设计了名为<a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/GVisor">gVisor (opens new window)</a>的轻量级虚拟化方案，这个方案与此前由 Intel 发起的<a target="_blank" rel="noopener" href="https://katacontainers.io/">Kata Containers (opens new window)</a>的思路异曲同工。目的都是解决容器共享操作系统内核而导致隔离性不足的安全缺陷，做法都是为每个容器提供了一个独立的虚拟 Linux 内核，譬如 gVisor 是用 Golang 实现了一个名为<a target="_blank" rel="noopener" href="https://github.com/google/gvisor/tree/master/pkg/sentry">Sentry (opens new window)</a>的能够提供传统操作系统内核的能力的进程，严格来说无论是 gVisor 还是 Kata Containers，尽管披着容器运行时的外衣，但本质上都是轻量级虚拟机。</li>
</ul>
<h1 id="网络"><a href="#网络" class="headerlink" title="网络"></a>网络</h1><figure class="highlight autohotkey"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><code class="hljs autohotkey">Endpoint 在不同场景下的含义如下：<br><br><span class="hljs-number">1</span>. Spring Security OAuth2：在Spring Security OAuth2中，`Endpoint`代表OAuth2授权服务器的端点地址，例如`/token`和`/authorize`等。这些端点允许客户端应用程序与OAuth2授权服务器交互以获取访问令牌等信息。<br><br><span class="hljs-number">2</span>. 容器：在容器的上下文中，`Endpoint`通常指容器内运行的应用程序接受网络请求的地址，例如，Web应用程序的`/api/v1`端点或自定义的健康检查端点。<br><br><span class="hljs-number">3</span>. 云原生：在云原生中，`Endpoint`通常指容器、微服务或云原生应用程序公开的API接口。这些接口可以用于与其他服务或应用程序交互，例如，通过HTTP API接口请求资源或创建和管理Kubernetes Pod容器。<br><br><span class="hljs-number">4</span>. Linux：在Linux上下文中，`Endpoint`可能是指内核对系统资源（如文件、进程、网络连接等）的抽象访问点。<br><br><span class="hljs-number">5</span>. 计算机网络：在计算机网络中，`Endpoint`通常指通信协议中涉及的通信点或终端，例如，客户端和服务器之间的连接端点或路由器和交换机的接口。<br><br><span class="hljs-number">6</span>. 网络模型：在异构网络环境中，`Endpoint`用于在模型中标识和管理网络中的各种连接和终端，例如，在SDN（软件定义网络）和NFV（网络功能虚拟化）中。<br><br><span class="hljs-number">7</span>. Kubernetes：在Kubernetes中，`Endpoint`代表一个服务IP地址和端口，该IP地址和端口将流量代理到多个Kubernetes Pod容器或其他服务。<br><br>综上所述，`Endpoint`是一个通用的术语，其含义依赖于不同的应用程序、系统和场景。无论是在Spring Security OAuth2、容器、云原生、Linux、计算机网络、网络模型还是Kubernetes中，`Endpoint`都是用于标识和访问不同API、资源或网络连接的表述性概念。<br></code></pre></td></tr></table></figure>





<h2 id="CNI-x2F-CNM"><a href="#CNI-x2F-CNM" class="headerlink" title="CNI &#x2F; CNM"></a>CNI &#x2F; CNM</h2><p>CNI和CNM都是用于容器网络方面的技术。</p>
<p>CNI是Container Networking Interface（容器网络接口）的缩写，是一种用于容器网络的独立桥接技术。CNI允许多种容器运行时使用相同的网络体系结构和网络插件，从而方便容器网络之间的互操作性和扩展性。CNI使用插件架构，提供标准化的API和数据格式，从而可以轻松地为容器设计和实现新的网络插件。常见的CNI插件包括Bridge、Calico、Flannel等。</p>
<p>相比之下，CNM是Docker的容器网络模型（Container Network Model）的缩写，它是Docker在容器网络方面的标准。CNM通过使用网络驱动程序和容器的命名空间机制来实现容器网络。CNM提供了多种内置网络驱动程序和自定义网络驱动程序的支持，并支持容器在单一或多个网络中运行的多种拓扑。CNM网络驱动程序包括bridge、host、macvlan、overlay等。</p>
<p>简言之，CNI和CNM都是容器网络方面的技术，CNI是较为通用的桥接网络方案，支持多个容器运行时；而CNM是Docker原生的容器网络技术，提供多种内置网络驱动程序，适用于Docker生态环境。</p>
<h3 id="Docker的三种网路模式"><a href="#Docker的三种网路模式" class="headerlink" title="Docker的三种网路模式"></a>Docker的三种网路模式</h3><p>Docker网络有三种模式：bridge、host和none。</p>
<ol>
<li><p>Bridge模式：Bridge是Docker默认的网络模式。在Bridge网络中，Docker将创建一个名为<code>docker0</code>的虚拟网络桥接器，并分配一个自动分配的IP段给桥接器。每个容器都将被分配一个从该网段中分配的IP地址。容器之间可以进行通信，但与容器之外的主机的通信需要进行端口映射。因此，该模式适用于简单的应用程序，例如Web应用程序和数据库等。</p>
</li>
<li><p>Host模式：在Host模式下，容器与主机使用相同的网络命名空间，可以使用主机的IP地址和端口进行通信。该模式适用于需要最大化网络性能的应用程序，例如高流量Web应用程序和数据库等。</p>
</li>
<li><p>None模式：在None模式下，容器没有默认的网络接口，并且无法直接与外界通信，需要手动创建和配置网络。该模式适用于不需要网络连接的容器，例如调试或数据存储等。</p>
</li>
</ol>
<p>综上所述，Bridge模式是Docker的默认网络模式，在容器之间隔离网络，但可以使用端口映射与主机或其他容器通信。Host模式允许容器与主机共享网络命名空间，以获得更好的网络性能，适用于高性能应用程序。None模式适用于不需要网络连接的容器。选择适当的网络模式取决于您的容器应用程序所需的网络连接方式以及需要的网络性能和安全性等要素。</p>
<h3 id="docker-本地-x2F-全局-网络"><a href="#docker-本地-x2F-全局-网络" class="headerlink" title="docker 本地 &#x2F; 全局 网络"></a>docker 本地 &#x2F; 全局 网络</h3><p>在Docker中，本地网络和全局网络的实现方式可以分为以下两类:</p>
<ol>
<li>本地网络实现方式：</li>
</ol>
<ul>
<li><p>Bridge网络：Bridge网络是Docker的默认网络模式，它允许容器通过一个虚拟网桥连接到宿主机的物理网络，可以通过创建自定义桥接器来扩展这个网络。</p>
</li>
<li><p>Host网络：Host网络模式允许容器和宿主机共享一个网络命名空间，所有容器都可以被视为宿主机上的应用程序。Host网络性能更高，但缺乏网络隔离。</p>
</li>
<li><p>Macvlan网络：Macvlan网络模式允许将Docker容器接入物理网络，每个容器可以使用独立的MAC地址和IP地址。</p>
</li>
</ul>
<ol start="2">
<li>全局网络实现方式：</li>
</ol>
<ul>
<li><p>Overlay网络：这是一种基于Docker的多主机容器网络模型，可以在多个Docker宿主机上创建一个虚拟的覆盖网络，使用VXLAN技术将多个Docker宿主机的容器连接在一起，并将这些容器放置在同一个Layer-2网络中。</p>
</li>
<li><p>Weaveworks Flannel：这是一种简单的软件定义网络（SDN）解决方案，旨在为Docker容器提供多主机SDN功能。</p>
</li>
<li><p>Project Calico：这是一个开放源码的vNetworking和网络安全项目，它允许在一个分布式环境中提供容器网络。</p>
</li>
</ul>
<p>综上所述，本地网络和全局网络的实现方式包括Bridge、Host和Macvlan网络模式，以及Overlay、Flannel和Calico等全局网络模式。选取合适的网络模式，取决于应用程序的特性、安全性和性能需求，还需要考虑Docker的网络单一主机内还是在跨多个主机之间等方面的限制。</p>
<h3 id="Overlay-docker"><a href="#Overlay-docker" class="headerlink" title="Overlay docker"></a>Overlay docker</h3><p>Overlay网络是一种基于Docker的多主机容器网络模型，可以在多个Docker宿主机上创建一个虚拟的覆盖网络。在Overlay网络中，Docker宿主机可以连接到同一个租户网络中，这个租户网络可以跨越多个Docker宿主机。Overlay网络使用VXLAN技术将多个Docker宿主机的容器连接在一起，并将这些容器放置在同一个Layer-2网络中。</p>
<p>Overlay网络具有以下特点：</p>
<ol>
<li><p>跨主机通信：Overlay网络支持在多个Docker宿主机上创建相同的虚拟网络，即可以在多个Docker宿主机上连接同一个租户网络。</p>
</li>
<li><p>路由优化：Overlay网络使用VXLAN技术将多个Docker宿主机上的容器连接在一起，并对IP流量进行分段和路由优化。</p>
</li>
<li><p>容错性：在Overlay网络中，当某个Docker宿主机出现故障或离线时，容器可以迁移到其他Docker宿主机上。</p>
</li>
<li><p>安全性：Overlay网络可以提供加密和身份验证等安全机制，从而保证容器之间的网络安全。</p>
</li>
</ol>
<p>Overlay网络适用于需要容器互联的分布式应用程序，例如数据分析、大数据应用程序和分布式数据库等。 在这些应用程序中，需要将多个容器放置在同一个虚拟网络中，并保证这些容器可以进行快速的、可靠的通信，Overlay网络可以满足这些需求。</p>
<h1 id="–-Linux-网络虚拟化"><a href="#–-Linux-网络虚拟化" class="headerlink" title="– Linux 网络虚拟化"></a>– Linux 网络虚拟化</h1><p><a target="_blank" rel="noopener" href="https://jingyecn.top:18080/immutable-infrastructure/network/linux-vnet.html">Linux 网络虚拟化 | 凤凰架构 (jingyecn.top)</a></p>
<h1 id="–-Kubernetes-1"><a href="#–-Kubernetes-1" class="headerlink" title="– Kubernetes"></a>– Kubernetes</h1><h1 id="Kubernetes-的存储设计"><a href="#Kubernetes-的存储设计" class="headerlink" title="Kubernetes 的存储设计"></a>Kubernetes 的存储设计</h1><h2 id="Volumn"><a href="#Volumn" class="headerlink" title="Volumn"></a>Volumn</h2><h2 id="docker的Volumn-和-Mount"><a href="#docker的Volumn-和-Mount" class="headerlink" title="docker的Volumn 和 Mount"></a>docker的Volumn 和 Mount</h2><p>Mount 是动词，表示将某个外部存储挂载到系统中，Volume 是名词，表示物理存储的逻辑抽象，目的是为物理存储提供有弹性的分割方式。</p>
<p>Docker 内建支持了三种挂载类型，分别是 Bind（<code>--mount type=bind</code>）、Volume（<code>--mount type=volume</code>）和 tmpfs（<code>--mount type=tmpfs</code>），如图 13-1 所示。其中 tmpfs 用于在内存中读写临时数据，与本节主要讨论的对象持久化存储并不相符，所以后面我们只着重关注 Bind 和 Volume 两种挂载类型。</p>
<p><img src="https://raw.githubusercontent.com/wherezy1/PicGOImageShack/main/images/%E4%B8%8B%E8%BD%BD.png?token=AUR75H2X3PHGB5QTDO72IZTEQU5MO" srcset="/img/loading.gif" lazyload alt="下载"></p>
<p>图 13-1 Docker 的三种挂载类型（图片来自<a target="_blank" rel="noopener" href="https://docs.docker.com/storage/">Docker 官网文档 (opens new window)</a>)</p>
<p>Bind Mount 是 Docker 最早提供的（发布时就支持）挂载类型，作用是把宿主机的某个目录（或文件）挂载到容器的指定目录（或文件）下，譬如以下命令中参数<code>-v</code>表达的意思就是将外部的 HTML 文档挂到 Nginx 容器的默认网站根目录下：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">docker run -v /icyfenix/html:/usr/share/nginx/html nginx:latest<br></code></pre></td></tr></table></figure>

<p>请注意，虽然命令中<code>-v</code>参数是<code>--volume</code>的缩写，但<code>-v</code>最初只是用来创建 Bind Mount 而不是创建 Volume Mount 的，这种迷惑的行为也并非 Docker 的本意，只是由于 Docker 刚发布时考虑得不够周全，随随便便就在参数中占用了“Volume”这个词，到后来真的需要扩展 Volume 的概念来支持 Volume Mount 时，前面的<code>-v</code>已经被用户广泛使用了，所以也就只得如此将就着继续用。从 Docker 17.06 版本开始，它在 Docker Swarm 中借用了<code>--mount</code>参数过来，这个参数默认创建的是 Volume Mount，可以通过明确的 type 子参数来指定另外两种挂载类型。上面命令可以等价于<code>--mount</code>版本如下形式：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">docker run --mount <span class="hljs-built_in">type</span>=<span class="hljs-built_in">bind</span>,<span class="hljs-built_in">source</span>=/icyfenix/html,destination=/usr/share/nginx/html nginx:latest<br></code></pre></td></tr></table></figure>

<p>从 Bind Mount 到 Volume Mount，实质是容器发展过程中对存储抽象能力提升的外在表现。从“Bind”这个名字，以及 Bind Mount 的实际功能可以合理地推测，Docker 最初认为“Volume”就只是一种“外部宿主机的磁盘存储到内部容器存储的映射关系”，但后来眉头一皱发现事情并没有那么简单：存储的位置并不局限只在外部宿主机、存储的介质并不局限只是物理磁盘、存储的管理也并不局限只有映射关系。</p>
<h2 id="静态存储分配"><a href="#静态存储分配" class="headerlink" title="静态存储分配"></a>静态存储分配</h2><h3 id="persistentVolumn-与-PersistentVolumnClaim"><a href="#persistentVolumn-与-PersistentVolumnClaim" class="headerlink" title="persistentVolumn 与 PersistentVolumnClaim"></a>persistentVolumn 与 PersistentVolumnClaim</h3><p>Kubernetes 官方给出的概念定义也特别强调了 PersistentVolume 是由管理员（运维人员）负责维护的，用户（开发人员）通过 PersistentVolumeClaim 来匹配到合乎需求的 PersistentVolume。</p>
<p>PersistentVolume &amp; PersistentVolumeClaim</p>
<p>A PersistentVolume （PV） is a piece of storage in the cluster that has been provisioned <strong>by an administrator</strong>.<br>A PersistentVolumeClaim （PVC） is a request for storage <strong>by a user</strong>.</p>
<p>PersistentVolume 是由管理员负责提供的集群存储。<br>PersistentVolumeClaim 是由用户负责提供的存储请求。</p>
<h3 id="使用："><a href="#使用：" class="headerlink" title="使用："></a>使用：</h3><p>两者配合工作的具体过程如下。</p>
<ol>
<li><p>管理员准备好要使用的存储系统，它应是某种网络文件系统（NFS）或者云储存系统，一般来说应该具备跨主机共享的能力。</p>
</li>
<li><p>管理员根据存储系统的实际情况，手工预先分配好若干个 PersistentVolume，并定义好每个 PersistentVolume 可以提供的具体能力。譬如以下例子所示：</p>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs yaml"><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">v1</span><br><span class="hljs-attr">kind:</span> <span class="hljs-string">PersistentVolume</span><br><span class="hljs-attr">metadata:</span><br>  <span class="hljs-attr">name:</span> <span class="hljs-string">nginx-html</span><br><span class="hljs-attr">spec:</span><br>  <span class="hljs-attr">capacity:</span><br>    <span class="hljs-attr">storage:</span> <span class="hljs-string">5Gi</span>                          <span class="hljs-comment"># 最大容量为5GB</span><br>  <span class="hljs-attr">accessModes:</span><br>    <span class="hljs-bullet">-</span> <span class="hljs-string">ReadWriteOnce</span>                       <span class="hljs-comment"># 访问模式为RWO</span><br>  <span class="hljs-attr">persistentVolumeReclaimPolicy:</span> <span class="hljs-string">Retain</span>	  <span class="hljs-comment"># 回收策略是Retain</span><br>  <span class="hljs-attr">nfs:</span>                                    <span class="hljs-comment"># 存储驱动是NFS</span><br>    <span class="hljs-attr">path:</span> <span class="hljs-string">/html</span><br>    <span class="hljs-attr">server:</span> <span class="hljs-number">172.17</span><span class="hljs-number">.0</span><span class="hljs-number">.2</span><br></code></pre></td></tr></table></figure>

<p>以上 YAML 中定义的存储能力具体为：</p>
<ul>
<li>存储的最大容量是 5GB。</li>
<li>存储的访问模式是“只能被一个节点读写挂载”（ReadWriteOnce，RWO），另外两种可选的访问模式是“可以被多个节点以只读方式挂载”（ReadOnlyMany，ROX）和“可以被多个节点读写挂载”（ReadWriteMany，RWX）。</li>
<li>存储的回收策略是 Retain，即在 Pod 被销毁时并不会删除数据。另外两种可选的回收策略分别是 Recycle ，即在 Pod 被销毁时，由 Kubernetes 自动执行<code>rm -rf /volume/*</code>这样的命令来自动删除资料；以及 Delete，它让 Kubernetes 自动调用 AWS EBS、GCE PersistentDisk、OpenStack Cinder 这些云存储的删除指令。</li>
<li>存储驱动是 NFS，其他常见的存储驱动还有 AWS EBS、GCE PD、iSCSI、RBD（Ceph Block Device）、GlusterFS、HostPath，等等。</li>
</ul>
</li>
<li><p>用户根据业务系统的实际情况，创建 PersistentVolumeClaim，声明 Pod 运行所需的存储能力。譬如以下例子所示：</p>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs yaml"><span class="hljs-attr">kind:</span> <span class="hljs-string">PersistentVolumeClaim</span><br><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">v1</span><br><span class="hljs-attr">metadata:</span><br>  <span class="hljs-attr">name:</span> <span class="hljs-string">nginx-html-claim</span><br><span class="hljs-attr">spec:</span><br>  <span class="hljs-attr">accessModes:</span><br>    <span class="hljs-bullet">-</span> <span class="hljs-string">ReadWriteOnce</span>    <span class="hljs-comment"># 支持RWO访问模式</span><br>  <span class="hljs-attr">resources:</span><br>    <span class="hljs-attr">requests:</span><br>      <span class="hljs-attr">storage:</span> <span class="hljs-string">5Gi</span>     <span class="hljs-comment"># 最小容量5GB</span><br></code></pre></td></tr></table></figure>

<p>以上 YAML 中声明了要求容量不得小于 5GB，必须支持 RWO 的访问模式。</p>
</li>
<li><p>Kubernetes 创建 Pod 的过程中，会根据系统中 PersistentVolume 与 PersistentVolumeClaim 的供需关系对两者进行撮合，如果系统中存在满足 PersistentVolumeClaim 声明中要求能力的 PersistentVolume 则撮合成功，将它们绑定。如果撮合不成功，Pod 就不会被继续创建，直至系统中出现新的或让出空闲的 PersistentVolume 资源。</p>
</li>
<li><p>以上几步都顺利完成的话，意味着 Pod 的存储需求得到满足，继续 Pod 的创建过程，整个过程如图 13-4 所示。</p>
</li>
</ol>
<p><img src="https://jingyecn.top:18080/assets/img/pv-pvc.74b08dd6.png" srcset="/img/loading.gif" lazyload alt="img"></p>
<p>​				图 13-4 PersistentVolumeClaim 与 PersistentVolume 运作过程（图片来自《<a target="_blank" rel="noopener" href="https://www.manning.com/books/kubernetes-in-action">Kubernetes in Action (opens new window)</a>》)</p>
<h4 id="问题"><a href="#问题" class="headerlink" title="问题"></a>问题</h4><p>Kubernetes 对 PersistentVolumeClaim 与 PersistentVolume 撮合的结果是产生一对一的绑定关系，“一对一”的意思是 PersistentVolume 一旦绑定在某个 PersistentVolumeClaim 上，直到释放以前都会被这个 PersistentVolumeClaim 所独占，不能再与其他 PersistentVolumeClaim 进行绑定。这意味着即使 PersistentVolumeClaim 申请的存储空间比 PersistentVolume 能够提供的要少，依然要求整个存储空间都为该 PersistentVolumeClaim 所用，这有可能会造成资源的浪费。譬如，某个 PersistentVolumeClaim 要求 3GB 的存储容量，当前 Kubernetes 手上只剩下一个 5GB 的 PersistentVolume 了，此时 Kubernetes 只好将这个 PersistentVolume 与申请资源的 PersistentVolumeClaim 进行绑定，平白浪费了 2GB 空间。假设后续有另一个 PersistentVolumeClaim 申请 2GB 的存储空间，那它也只能等待管理员分配新的 PersistentVolume，或者有其他 PersistentVolume 被回收之后才被能成功分配。</p>
<h2 id="动态存储分配"><a href="#动态存储分配" class="headerlink" title="动态存储分配"></a>动态存储分配</h2><p>关键词：Provisioner、StorageClass、</p>
<p>所谓的动态存储分配方案，是指在用户声明存储能力的需求时，不是期望通过 Kubernetes 撮合来获得一个管理员人工预置的 PersistentVolume，而是由特定的资源分配器（Provisioner）自动地在存储资源池或者云存储系统中分配符合用户存储需要的 PersistentVolume，然后挂载到 Pod 中使用，完成这项工作的资源被命名为 StorageClass，它的具体工作过程如下：</p>
<ol>
<li><p>管理员根据储系统的实际情况，先准备好对应的 Provisioner。Kubernetes 官方已经提供了一系列<a target="_blank" rel="noopener" href="https://kubernetes.io/docs/concepts/storage/storage-classes/">预置的 In-Tree Provisioner (opens new window)</a>，放置在<code>kubernetes.io</code>的 API 组之下。其中部分 Provisioner 已经有了官方的 CSI 驱动，譬如 vSphere 的 Kubernetes 自带驱动为<code>kubernetes.io/vsphere-volume</code>，VMware 的官方驱动为<code>csi.vsphere.vmware.com</code>。</p>
</li>
<li><p>管理员不再是手工去分配 PersistentVolume，而是根据存储去配置 StorageClass。Pod 是可以动态扩缩的，而存储则是相对固定的，哪怕使用的是具有扩展能力的云存储，也会将它们视为存储容量、IOPS 等参数可变的固定存储来看待，譬如你可以将来自不同云存储提供商、不同性能、支持不同访问模式的存储配置为各种类型的 StorageClass，这也是它名字中“Class”（类型）的由来，譬如以下例子所示：</p>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs yaml"><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">storage.k8s.io/v1</span><br><span class="hljs-attr">kind:</span> <span class="hljs-string">StorageClass</span><br><span class="hljs-attr">metadata:</span><br>  <span class="hljs-attr">name:</span> <span class="hljs-string">standard</span><br><span class="hljs-attr">provisioner:</span> <span class="hljs-string">kubernetes.io/aws-ebs</span>  <span class="hljs-comment">#AWS EBS的Provisioner</span><br><span class="hljs-attr">parameters:</span><br>  <span class="hljs-attr">type:</span> <span class="hljs-string">gp2</span><br><span class="hljs-attr">reclaimPolicy:</span> <span class="hljs-string">Retain</span><br></code></pre></td></tr></table></figure>
</li>
<li><p>用户依然通过 PersistentVolumeClaim 来声明所需的存储，但是应在声明中明确指出该由哪个 StorageClass 来代替 Kubernetes 处理该 PersistentVolumeClaim 的请求，譬如以下例子所示：</p>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs yaml"><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">v1</span><br><span class="hljs-attr">kind:</span> <span class="hljs-string">PersistentVolumeClaim</span><br><span class="hljs-attr">metadata:</span><br>  <span class="hljs-attr">name:</span> <span class="hljs-string">standard-claim</span><br><span class="hljs-attr">spec:</span><br>  <span class="hljs-attr">accessModes:</span><br>  <span class="hljs-bullet">-</span> <span class="hljs-string">ReadWriteOnce</span><br>  <span class="hljs-attr">storageClassName:</span> <span class="hljs-string">standard</span>  <span class="hljs-comment">#明确指出该由哪个StorageClass来处理该PersistentVolumeClaim的请求</span><br>  <span class="hljs-attr">resource:</span><br>    <span class="hljs-attr">requests:</span><br>      <span class="hljs-attr">storage:</span> <span class="hljs-string">5Gi</span><br></code></pre></td></tr></table></figure>
</li>
<li><p>如果 PersistentVolumeClaim 中要求的 StorageClass 及它用到的 Provisioner 均是可用的话，那这个 StorageClass 就会接管掉原本由 Kubernetes 撮合 PersistentVolume 与 PersistentVolumeClaim 的操作，按照 PersistentVolumeClaim 中声明的存储需求，自动产生出满足该需求的 PersistentVolume 描述信息，并发送给 Provisioner 处理。</p>
</li>
<li><p>Provisioner 接收到 StorageClass 发来的创建 PersistentVolume 请求后，会操作其背后存储系统去分配空间，如果分配成功，就生成并返回符合要求的 PersistentVolume 给 Pod 使用。</p>
</li>
<li><p>以上几步都顺利完成的话，意味着 Pod 的存储需求得到满足，继续 Pod 的创建过程，整个过程如图 13-5 所示。</p>
</li>
</ol>
<p><img src="https://jingyecn.top:18080/assets/img/storage-class.4d160b17.png" srcset="/img/loading.gif" lazyload alt="img"></p>
<p>图 13-5 StorageClass 运作过程（图片来自《<a target="_blank" rel="noopener" href="https://www.manning.com/books/kubernetes-in-action">Kubernetes in Action (opens new window)</a>》)</p>
<p>Dynamic Provisioning 与 Static Provisioning 并不是各有用途的互补设计，而是对同一个问题先后出现的两种解决方案。你完全可以只用 Dynamic Provisioning 来实现所有的 Static Provisioning 能够实现的存储需求，包括那些不需要动态分配的场景</p>
<h1 id="容器存储与生态"><a href="#容器存储与生态" class="headerlink" title="容器存储与生态"></a>容器存储与生态</h1><h2 id="in-tree和-out-of-tree"><a href="#in-tree和-out-of-tree" class="headerlink" title="in-tree和 out-of-tree"></a>in-tree和 out-of-tree</h2><p>在 Kubernetes 中，In-Tree 插件是指被标记为已稳定并内置在核心代码中的插件，而 Out-of-Tree 插件是指托管在 independent 项目中并与 Kubernetes API 耦合的插件。</p>
<p>应用场景方面，In-Tree 插件提供稳定的内置功能，包括 kubelet plugins、FlexVolume 和 CSI drivers 等，它们都内置在 Kubernetes 使用的代码库中，易于使用和部署；而 Out-of-Tree 插件通常更加专业、通用和灵活，它们可以为不同的场景和应用提供更加细粒度的控制和自定义配置，比如 Metrics Server、KubeDB等。它们通常作为 Kubernetes 插件来使用，并且可以在 Kubernetes 社区管理的托管项目中进行开发和维护。</p>
<p>具体介绍两种方式的含义：</p>
<ul>
<li><p>In-Tree 插件: In-Tree 插件是指将 Kubernetes 插件编写成 Kubernetes 核心代码库的一部分。这种方式使得插件开发人员可以访问 Kubernetes 源代码，并且该插件的发布、部署和维护非常方便。这种插件通常需要经过审核才能被合并到 Kubernetes 的代码库中，并且受到 Kubernetes 团队的长期支持。由于 In-Tree 插件内置在 Kubernetes 中，因此它们的稳定性、性能和安全性非常高。</p>
</li>
<li><p>Out-of-Tree 插件: Out-of-Tree 插件则是指将 Kubernetes 插件作为独立的项目开发、测试并发布。这种方式使得插件开发人员可以以更加灵活和自由的方式开发插件，同时，它们可以对 Kubernetes 的版本和配置进行更加细粒度的控制，以确保其与 Kubernetes API 的兼容性。此外，由于 Out-of-Tree 插件和 Kubernetes 开发具有松散的耦合度，因此它们也可以更加灵活和快速地实现一些非常特定的、满足某些应用需求的功能。</p>
</li>
</ul>
<p>综上所述，两种 Kubernetes 插件的方式各有优缺点，开发人员可以根据应用场景的实际需求，选择适当的插件，并根据需求与可行性选择对应的方式进行开发、测试和部署。</p>
<h2 id="三种存储方式"><a href="#三种存储方式" class="headerlink" title="三种存储方式"></a>三种存储方式</h2><p>目前出现过的存储系统和设备均可以划分到块存储、文件存储和对象存储这三种存储类型之中，划分的根本依据其实并非各种存储是如何储存数据的——那完全是存储系统私有的事情，更合理的划分依据是各种存储提供何种形式的接口供外部访问数据，不同的外部访问接口将反过来影响到存储的内部结构、性能与功能表现。虽然块存储、文件存储和对象存储可以彼此协同工作，但它们各自都有自己明确的擅长领域与优缺点，理解它们的工作原理，因地制宜地选择最适合的存储才能让系统达到最佳的工作状态。笔者按照它们出现的时间顺序分别介绍如下：</p>
<ul>
<li><p><strong>块存储</strong>：块存储是数据存储的最古老形式，数据都储存在固定长度的一个或多个<a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Block_(data_storage)">块 (opens new window)</a>（Block）中，想要读写访问数据，就必须使用与存储相匹配的协议（SCSI、SATA、SAS、FCP、FCoE、iSCSI……）来进行的。如果你是按顺序阅读本书内容的话，那笔者建议你类比上一章网络通讯中<a target="_blank" rel="noopener" href="https://jingyecn.top:18080/immutable-infrastructure/network/linux-vnet.html#%E7%BD%91%E7%BB%9C%E9%80%9A%E8%AE%AF%E6%A8%A1%E5%9E%8B">网络栈</a>的数据流动过程，把存储设备中由块构成的信息流与网络设备中由数据包构成的信息流进行对比，事实上，像 iSCSI 这种协议真的就是建设在 TCP&#x2F;IP 网络之上，上层以 SCSI 作为应用层协议对外提供服务的。</p>
<p>我们熟悉的硬盘就是最经典的块存储设备，以机械硬盘为例，一个块就是一个扇区，大小通常在 512 Bytes 至 4096 Bytes 之间。老式机械硬盘用<a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Cylinder-head-sector">柱面-磁头-扇区号 (opens new window)</a>（Cylinder-Head-Sector，CHS）组成的编号进行寻址，现代机械硬盘只用一个<a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Logical_block_addressing">逻辑块编号 (opens new window)</a>（Logical Block Addressing，LBA）进行寻址。为了便于管理，硬盘通常会以多个块（这些块甚至可以来自不同的物理设备，譬如磁盘阵列的情况）来组成一个逻辑分区（Partition），将分区进行<a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Disk_formatting#High-level_formatting">高级格式化 (opens new window)</a>之后就形成了卷（Volume），这便与上节“<a target="_blank" rel="noopener" href="https://jingyecn.top:18080/immutable-infrastructure/storage/storage-evolution.html">Kubernetes 存储设计</a>”中提到“Volume 是源于操作系统的概念”衔接了起来。</p>
<p>块存储由于贴近底层硬件，没有文件、目录、访问权限等的牵绊，所以性能通常都是最优秀的，吞吐量高，延迟低。尽管人类作为信息系统的最终用户，并不会直接面对块来操作数据，多数应用程序也是基于文件而不是块来读写数据的，但是操作系统内核中许多地方就直接通过<a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Device_file#BLOCKDEV">块设备 (opens new window)</a>（Block Device）接口来访问硬盘，一些追求 I&#x2F;O 性能的软件，譬如高性能的数据库也会支持直接读写块设备以提升磁盘 I&#x2F;O。块存储的特点是具有排它性，一旦块设备被某个客户端挂载后，其它客户端就无法再访问上面的数据了，因此，Kubernetes 中挂载的块存储大多访问模式都要求必须是 RWO（ReadWriteOnce）的。</p>
</li>
<li><p><strong>文件存储</strong>：文件存储是最贴近人类用户的数据存储形式，数据存储在长度不固定的文件之中，用户可以针对文件进行新增、写入、追加、移动、复制、删除、重命名等各种操作，通常文件存储还会提供有文件查找、目录管理、权限控制等额外的高级功能。文件存储的访问不像块存储那样有五花八门的协议，<a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/POSIX">POSIX (opens new window)</a>接口（Portable Operating System Interface，POSIX）已经成为了事实标准，被各种商用的存储系统和操作系统共同支持。具体 POSIX 的文件操作接口笔者就不去举例罗列了，你不妨类比 Linux 下的各种文件管理命令来自行想象一下。</p>
<p>绝大多数传统的文件存储都是基于块存储之上去实现的，“文件”这个概念的出现是因为“块”对人类用户来说实在是过于难以使用、难以管理了。可以近似地认为文件是由块所组成的更高级存储单位。对于固定不会发生变动的文件，直接让每个文件连续占用若干个块，在文件头尾加入标志区分即可，磁带、CD-ROM、DVD-ROM 就采用了由连续块来构成文件的存储方案；但对于可能发生变动的场景，就必须考虑如何跨多个不连续的块来构成为文件。这种需求在数据结构角度看只需在每个块中记录好下一个块的地址，形成链表结构即可满足。但是链表的缺点是只能依次顺序访问，这样访问文件中任何内容都要从头读取多个块，显然过于低效了。真正被广泛运用的解决方案是把形成链表的指针整合起来统一存放，这便形成了<a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/File_Allocation_Table">文件分配表 (opens new window)</a>（File Allocation Table，FAT）的概念。既然已经有了专门组织块结构来构成文件的分配表，那在表中再加入其他控制信息，就能很方便地扩展出更多的高级功能，譬如除了文件占用的块地址信息外，加上文件的逻辑位置就形成了目录，加上文件的访问标志就形成了权限，还可以再加上文件的名称、创建时间、所有者、修改者等一系列的元数据信息来构成其他应用形式。人们把定义文件分配表应该如何实现、储存哪些信息、提供什么功能的标准称为<a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/File_system">文件系统 (opens new window)</a>（File System），FAT32、NTFS、exFAT、ext2&#x2F;3&#x2F;4、XFS、BTRFS 等都是很常用的文件系统。而前面介绍存储插件接口时提到的对分区进行高级格式化操作，实际上就是在初始化一套空白的文件系统，供后续用户与应用程序访问。</p>
<p>文件存储相对于块存储来说是更高层次的存储类型，加入目录、权限等元素后形成的树状结构以及路径访问方式方便了人类理解、记忆和访问；文件系统能够提供哪个进程打开或正在读写某个文件的信息，这也有利于文件的共享处理。但在另一方面，计算机需要把路径进行分解，然后逐级向下查找，最后才能查找到需要的文件，要从文件分配表中确定具体数据存储的位置，要判断文件的访问权限，要记录每次修改文件的用户与时间，这些额外操作对于性能产生负面影响也是无可避免的，因此，如果一个系统选择不采用文件存储的话，那磁盘 I&#x2F;O 性能一般就是最主要的决定因素。</p>
</li>
<li><p><strong>对象储存</strong>：<a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Object_storage">对象存储 (opens new window)</a>是相对较新的数据存储形式，是一种随着云数据中心的兴起而发展起来的存储，是以非结构化数据为目标的存储方案。这里的“对象”可以理解为一个元数据及与其配对的一个逻辑数据块的组合，元数据提供了对象所包含的上下文信息，譬如数据的类型、大小、权限、创建人、创建时间，等等，数据块则存储了对象的具体内容。你也可以简单地理解为数据和元数据这两样东西共同构成了一个对象。每个对象都有属于自己的全局唯一标识，这个标识会直接开放给最终用户使用，作为访问该对象的主要凭据，通常会是 UUID 的形式。对象存储的访问接口就是根据该唯一标识，对逻辑数据块进行的读写删除操作，通常接口都会十分简单，甚至连修改操作都不会提供。</p>
<p>对象存储基本上只会在分布式存储系统之上去实现，由于对象存储天生就有明确的“元数据”概念，不必依靠文件系统来提供数据的描述信息，因此，完全可以将一大批对象的元数据集中存放在某一台（组）服务器上，再辅以多台 OSD（Object Storage Device）服务器来存储对象的数据块部分。当外部要访问对象时，多台 OSD 能够同时对外发送数据，因此对象存储不仅易于共享、容量庞大，还能提供非常高的吞吐量。不过，由于需要先经过元数据查询确定 OSD 存放对象的确切位置，该过程可能涉及多次网络传输，延迟方面就会表现得相对较差。</p>
<p>由于对象的元数据仅描述对象本身的信息，与其他对象都没有关联，换而言之每个对象都是相互独立的，自然也就不存在目录的概念，可见对象存储天然就是扁平化的，与软件系统中很常见的 K&#x2F;V 访问相类似，不过许多对象存储会提供 Bucket 的概念，用户可以在逻辑上把它看作是“单层的目录”来使用。由于对象存储天生的分布式特性，以及极其低廉的扩展成本，使它很适合于<a target="_blank" rel="noopener" href="https://jingyecn.top:18080/architect-perspective/general-architecture/diversion-system/cdn.html">CDN</a>一类的应用，拿来存放图片、音视频等媒体内容，以及网页、脚本等静态资源。</p>
</li>
</ul>
<h1 id="资源与调度"><a href="#资源与调度" class="headerlink" title="资源与调度"></a>资源与调度</h1><p>关键词：node、pod、predicate算法、etcd</p>
<figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs routeros">etcd 是一个高可用的键值存储系统，被用作分布式系统的共享配置存储、服务发现、集群协调等场景。etcd最初是由 CoreOS 发起的，现在由CNCF维护，是一个开源、分布式、一致的键值存储系统，完全兼容Raft算法。etcd 提供了一组简单的API来设置、查询和删除数据，它使用类似于文件系统的层次结构存储数据，可以被看作是一个分布式的键值存储的数据库。<br><br>etcd运行在集群中的多个节点上，集群的每个节点都可以响应用户的读写请求，使用一种分布式共识算法（如Raft算法）来保证集群中的每个节点状态是一致的。当其中一个节点收到客户端请求时，会同步信息给其他节点，然后等待大多数节点的反馈，就可以确认该操作是否生效。如果一个节点故障了，etcd 会通过选择新的领导者来继续服务。由于其高可用性和一致性，etcd 在 Kubernetes 中被广泛用于存储 k8s 集群状态和配置信息，例如API<span class="hljs-built_in"> Server </span>的配置参数、Pod 和<span class="hljs-built_in"> Service </span>的信息、集群状态等。<br><br>总之，etcd 作为分布式系统的KV存储服务，通过提供简单的键值读写API，并使用 Raft 算法来保证强一致性、高可用性和分区容忍性，支持各种各样的场景和应用需求，并被广泛应用于各种分布式系统中。<br></code></pre></td></tr></table></figure>



<h2 id="默认调度器"><a href="#默认调度器" class="headerlink" title="默认调度器"></a>默认调度器</h2><p>本节的最后一部分，我们回过头来探讨开篇提出的问题：Kubernetes 是如何撮合 Pod 与 Node 的，这其实也是最困难的一个问题。调度是为新创建出来的 Pod 寻找到一个最恰当的宿主机节点去运行它，这句话里就包含有“运行”和“恰当”两个调度中关键过程，它们具体是指：</p>
<ol>
<li><strong>运行</strong>：从集群所有节点中找出一批剩余资源可以满足该 Pod 运行的节点。为此，Kubernetes 调度器设计了一组名为 Predicate 的筛选算法。</li>
<li><strong>恰当</strong>：从符合运行要求的节点中找出一个最适合的节点完成调度。为此，Kubernetes 调度器设计了一组名为 Priority 的评价算法。</li>
</ol>
<p>这两个算法的具体内容稍后笔者会详细解释，这里要先说明白一点：在几个、十几个节点的集群里进行调度，调度器怎么实现都不会太困难，但是对于数千个乃至更多节点的大规模集群，要实现高效的调度就绝不简单。请你想象一下，若一个由数千节点组成的集群，每次 Pod 的创建都必须依据各节点的实时资源状态来确定调度的目标节点，然而各节点的资源是随着程序运行无时无刻都在变动的，资源状况只有它本身才清楚，如果每次调度都要发生数千次的远程访问来获取这些信息的话，那压力与耗时都难以降下来。结果不仅会令调度器成为集群管理的性能瓶颈，还会出现因耗时过长，某些节点上资源状况已发生变化，调度器的资源信息过时而导致调度结果不准确等问题。</p>
<p>Scheduler</p>
<p>Clusters and their workloads keep growing, and since the scheduler’s workload is roughly proportional to the cluster size, the scheduler is at risk of becoming a scalability bottleneck.</p>
<p>由于调度器的工作负载与集群规模大致成正比，随着集群和它们的工作负载不断增长，调度器很有可能会成为扩展性瓶颈所在。</p>
<p>—— <a target="_blank" rel="noopener" href="https://static.googleusercontent.com/media/research.google.com/zh-CN//pubs/archive/41684.pdf">Omega: Flexible, Scalable Schedulers for Large Compute Clusters (opens new window)</a>，Google</p>
<p>针对以上问题，Google 在论文《<a target="_blank" rel="noopener" href="https://static.googleusercontent.com/media/research.google.com/zh-CN//pubs/archive/41684.pdf">Omega: Flexible, Scalable Schedulers for Large Compute Clusters (opens new window)</a>》里总结了自身的经验，并参考了当时<a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Apache_Mesos">Apache Mesos (opens new window)</a>和<a target="_blank" rel="noopener" href="https://hadoop.apache.org/docs/r1.0.4/cn/hod.html">Hadoop on Demand (opens new window)</a>（HOD）的实现，提出了一种共享状态（Shared State）的双循环调度机制。这种调度机制后来不仅应用在 Google 的 Omega 系统（Borg 的下一代集群管理系统）中，也同样被 Kubernetes 继承了下来，它整体的工作流程如图 14-1 所示：</p>
<p><img src="https://raw.githubusercontent.com/wherezy1/PicGOImageShack/main/images/%E4%B8%8B%E8%BD%BD%20(1).png?token=AUR75HYFUW7PUZJREHST5P3EQXPEQ" srcset="/img/loading.gif" lazyload alt="下载 (1)"></p>
<p>图 14-1 状态共享的双循环</p>
<p>状态共享的双循环中第一个控制循环可被称为“Informer Loop”，它是一系列<a target="_blank" rel="noopener" href="https://godoc.org/k8s.io/client-go/informers">Informer (opens new window)</a>的集合，这些 Informer 持续监视 Etcd 中与调度相关资源（主要是 Pod 和 Node）的变化情况，一旦 Pod、Node 等资源出现变动，就会触发对应 Informer 的 Handler。Informer Loop 的职责是根据 Etcd 中的资源变化去更新调度队列（Priority Queue）和调度缓存（Scheduler Cache）中的信息，譬如当有新 Pod 生成，就将其入队（Enqueue）到调度队列中，如有必要，还会根据优先级触发上一节提到的插队和抢占操作。又譬如有新的节点加入集群，或者已有节点资源信息发生变动，Informer 也会将这些信息更新同步到调度缓存之中。</p>
<p>另一个控制循环可被称为“Scheduler Loop”，它的核心逻辑是不停地将调度队列中的 Pod 出队（Pop），然后使用 Predicate 算法进行节点选择。Predicate 本质上是一组节点过滤器（Filter），它根据预设的过滤策略来筛选节点，Kubernetes 中默认有三种过滤策略，分别是：</p>
<ul>
<li><strong>通用过滤策略</strong>：最基础的调度过滤策略，用来检查节点是否能满足 Pod 声明中需要的资源。譬如处理器、内存资源是否满足，主机端口与声明的 NodePort 是否存在冲突，Pod 的选择器或者<a target="_blank" rel="noopener" href="https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#node-affinity">nodeAffinity (opens new window)</a>指定的节点是否与目标相匹配，等等。</li>
<li><strong>卷过滤策略</strong>：与存储相关的过滤策略，用来检查节点挂载的 Volume 是否存在冲突（譬如将一个块设备挂载到两个节点上），或者 Volume 的<a target="_blank" rel="noopener" href="https://jingyecn.top:18080/distribution/connect/load-balancing.html#%E5%9C%B0%E5%9F%9F%E4%B8%8E%E5%8C%BA%E5%9F%9F">可用区域</a>是否与目标节点冲突，等等。在“<a target="_blank" rel="noopener" href="https://jingyecn.top:18080/immutable-infrastructure/storage/storage-evolution.html">Kubernetes 存储设计</a>”中提到的 Local PersistentVolume 的调度检查，便是在这里处理的。</li>
<li><strong>节点过滤策略</strong>：与宿主机相关的过滤策略，最典型的是 Kubernetes 的<a target="_blank" rel="noopener" href="https://kubernetes.io/docs/concepts/scheduling-eviction/taint-and-toleration/">污点与容忍度机制 (opens new window)</a>（Taints and Tolerations），譬如默认情况下 Kubernetes 会设置 Master 节点不允许被调度，这就是通过在 Master 中施加污点来避免的。之前提到的控制节点处于驱逐状态，或者在驱逐后一段时间不允许调度，也是在这个策略里实现的。</li>
</ul>
<p>Predicate 算法所使用的一切数据均来自于调度缓存，绝对不会去远程访问节点本身。只有 Informer Loop 与 Etcd 的监视操作才会涉及到远程调用，Scheduler Loop 中除了最后的异步绑定要发起一次远程的 Etcd 写入外，其余全部都是进程内访问，这一点是调度器执行效率的重要保证。</p>
<p>调度缓存就是两个控制循环的共享状态（Shared State），这样的设计避免了每次调度时主动去轮询所有集群节点，保证了调度器的执行效率。但是并不能完全避免因节点信息同步不及时而导致调度过程中实际资源发生变化的情况，譬如节点的某个端口在获取调度信息后、发生实际调度前被意外占用了。为此，当调度结果出来以后，kubelet 真正创建 Pod 以前，还必须执行一次 Admit 操作，在该节点上重新做一遍 Predicate 来进行二次确认。</p>
<p>经过 Predicate 算法筛选出来符合要求的节点集，会交给 Priorities 算法来打分（0-10 分）排序，以便挑选出“最恰当”的一个。“恰当”是带有主观色彩的词语，Kubernetes 也提供了不同的打分规则来满足不同的主观需求，譬如最常用的 LeastRequestedPriority 规则，它的计算公式是：</p>
<figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs text">score = (cpu((capacity-sum(requested))×10/capacity) + memory((capacity-sum(requested))×10/capacity))/2<br></code></pre></td></tr></table></figure>

<p>从公式上很容易看出这就是在选择处理器和内存空闲资源最多的节点，因为这些资源剩余越多，得分就越高。经常与它一起工作的是 BalancedResourceAllocation 规则，它的公式是：</p>
<figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs text">score = 10 - variance(cpuFraction,memoryFraction,volumeFraction)×10<br></code></pre></td></tr></table></figure>

<p>此公式中 cpuFraction、memoryFraction、volumeFraction 的含义分别是 Pod 请求的处理器、内存和存储资源占该节点上对应可用资源的比例，variance 函数的作用是计算资源之间的差距，差距越大，函数值越大。由此可知 BalancedResourceAllocation 规则的意图是希望调度完成后，所有节点里各种资源分配尽量均衡，避免节点上出现诸如处理器资源被大量分配、而内存大量剩余的尴尬状况。Kubernetes 内置的其他的评分规则还有 ImageLocalityPriority、NodeAffinityPriority、TaintTolerationPriority 等等，有兴趣的话可以阅读 Kubernetes 的源码，笔者就不再逐一解释了。</p>
<p>经过 Predicate 的筛选、Priorities 的评分之后，调度器已经选出了调度的最终目标节点，最后一步是通知目标节点的 kubelet 可以去创建 Pod 了。调度器并不会直接与 kubelet 通讯来创建 Pod，它只需要把待调度的 Pod 的<code>nodeName</code>字段更新为目标节点的名字即可，kubelet 本身会监视该值的变化来接手后续工作。不过，从调度器在 Etcd 中更新<code>nodeName</code>，到 kubelet 从 Etcd 中检测到变化，再执行 Admit 操作二次确认调度可行性，最后到 Pod 开始实际创建，这个过程可能会持续一段不短的时间，如果一直等待这些工作都完成了才宣告调度最终完成，那势必也会显著影响调度器的效率。实际上 Kubernetes 调度器采用了乐观绑定（Optimistic Binding）的策略来解决此问题，它会同步地更新调度缓存中 Pod 的<code>nodeName</code>字段，并异步地更新 Etcd 中 Pod 的<code>nodeName</code>字段，这个操作被称为绑定（Binding）。如果最终调度成功了，那 Etcd 与调度缓存中的信息最终必定会保持一致，否则，如果调度失败了，那将会由 Informer 来根据 Pod 的变动，将调度成功却没有创建成功的 Pod 清空<code>nodeName</code>字段，重新同步回调度缓存中，以便促使另外一次调度的开始。</p>
<p>最后，请注意笔者在这一个部分的小标题用的是“<strong>默认</strong>调度器”，这是强调以上行为仅是 Kubernetes 默认的行为。对调度过程的大部分行为，你都可以通过 Scheduler Framework 暴露的接口来进行扩展和自定义，如下图所示，绿色的部分就是 Scheduler Framework 暴露的扩展点。由于 Scheduler Framework 属于 Kubernetes 内部的扩展机制（通过 Golang 的 Plugin 机制来实现的，需静态编译），通用性与本章提到的其他扩展机制（CRI、CNI、CSI 那些）无法相提并论，属于较为高级的 Kubernetes 管理技能了，这里笔者仅在这里简单地提一下，就不多做介绍了。</p>
<p><img src="https://raw.githubusercontent.com/wherezy1/PicGOImageShack/main/images/context.758f5879.png?token=AUR75H23ATNE5HXHXVD5ZIDEQXPA2" srcset="/img/loading.gif" lazyload alt="context"></p>
<p>图 14-2 Scheduler Framework 的可扩展性（<a target="_blank" rel="noopener" href="https://medium.com/dev-genius/kubernetes-scheduling-system-f8705e7ee226">图片来源 (opens new window)</a>)</p>
<h1 id="–-服务网格"><a href="#–-服务网格" class="headerlink" title="– 服务网格"></a>– 服务网格</h1><p>服务网格（Service Mesh）</p>
<blockquote>
<p>A service mesh is a dedicated infrastructure layer for handling service-to-service communication. It’s responsible for the reliable delivery of requests through the complex topology of services that comprise a modern, cloud native application. In practice, the service mesh is typically implemented as an array of lightweight network proxies that are deployed alongside application code, without the application needing to be aware.</p>
<p>服务网格是一种用于管控服务间通信的的基础设施，职责是为现代云原生应用支持网络请求在复杂的拓扑环境中可靠地传递。在实践中，服务网格通常会以轻量化网络代理的形式来体现，这些代理与应用程序代码会部署在一起，对应用程序来说，它完全不会感知到代理的存在。</p>
</blockquote>
<h2 id="关键词："><a href="#关键词：" class="headerlink" title="关键词："></a>关键词：</h2><p>透明传输</p>
<figure class="highlight armasm"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs armasm">透明通信是指在不需要用户或应用感知的情况下，将通信的细节和逻辑隐藏起来的一种通信方式。因此，透明通信可以在单机、分布式和云原生等不同场景中应用，并具有不同的含义。<br><br>在单机场景中，透明通信主要指 TCP/<span class="hljs-built_in">IP</span> 协议中的传输控制协议（TCP）和用户数据报协议（UDP），用户或应用程序在进行网络通信时不需要感知这些协议的细节，只需要关注发送和接收数据包的高层 API。<br><br>在分布式场景中，透明通信通常指将分布式应用程序中的多个节点连接到一个虚拟网络上，并使它们能够像单个节点一样通信。这种方式通常使用诸如分布式共识算法、分布式锁等技术实现，可以大大提高分布式系统的稳定性和可靠性。<br><br>在云原生场景中，透明通信则更加注重应用程序和服务的可移植性和弹性。例如，在 Kubernetes 中，透明通信使得容器化的应用程序可以透明访问其他服务、挂载存储、进行服务发现等，而不需要进行任何额外的配置和设置。这种方式可以使得应用程序更加易于迁移、扩展和管理。<br><br>总的来说，透明通信作为一种通信方式，可以在单机、分布式和云原生等不同的场景中使用，并充分发挥分布式计算和云计算的优势，使得应用程序可以更加灵活、可扩展和高效地进行通信。<br></code></pre></td></tr></table></figure>



<h1 id="透明通信-涅槃"><a href="#透明通信-涅槃" class="headerlink" title="透明通信 - 涅槃"></a>透明通信 - 涅槃</h1><p><a target="_blank" rel="noopener" href="https://jingyecn.top:18080/immutable-infrastructure/mesh/communication.html">透明通信的涅槃 | 凤凰架构 (jingyecn.top)</a></p>
<h2 id="通信的成本"><a href="#通信的成本" class="headerlink" title="通信的成本"></a>通信的成本</h2><p>程序间通信作为分布式架构的核心内容，第一章“<a target="_blank" rel="noopener" href="https://jingyecn.top:18080/architecture/architect-history/">服务架构演进史</a>”中笔者就已从宏观角度讲述过它的演进过程。这节里，我们会从更微观更聚焦的角度，分析不同时期应用程序如何看待与实现通信方面的非功能性需求，如何做到可靠的通信的，通过以下五个阶段的变化，理解分布式服务的通信是如何逐步演化成本章主角服务网格。</p>
<ul>
<li><p><strong>第一阶段：将通信的非功能性需求视作业务需求的一部分，通信的可靠性由程序员来保障</strong>。<br>本阶段是软件企业刚刚开始尝试分布式时选择的早期技术策略。这类系统原本所具有的通信能力一般并不是作为系统功能的一部分被设计出来的，而是遇到问题后修补累积所形成的。开始时，系统往往只具最基本的网络 API，譬如集成了 OKHTTP、gRPC 这样库来访问远程服务，如果远程访问接收到异常，就编写对应的重试或降级逻辑去应对处理。在系统进入生产环境以后，遇到并解决的一个个通信问题，逐渐在业务系统中留下了越来越多关于通信的代码逻辑。这些通信的逻辑由业务系统的开发人员直接编写，与业务逻辑直接共处在一个进程空间之中，如图 15-1 所示（注：本图与后面一系列图片中，笔者均以“断路器”和“服务发现”这两个常见的功能来泛指所有的分布式通信所需的能力，实际上并不局限于这两个功能）。</p>
<p><img src="https://raw.githubusercontent.com/wherezy1/PicGOImageShack/main/images/image-20230611230524846.png?token=AUR75H3OXGJSISTN4Z4NF7TEQXRPC" srcset="/img/loading.gif" lazyload alt="image-20230611230524846">图 15-1 控制逻辑和业务逻辑耦合</p>
<p>这一阶段的主要矛盾是绝大多数擅长业务逻辑的开发人员都并不擅长处理通信方面的问题，要写出正确、高效、健壮的分布式通信代码，是一项专业性极强的工作。由此决定了大多数的普通软件企业都很难在这个阶段支撑起一个靠谱的分布式系统来。另一方面，把专业的通信功能强加于普通开发人员，无疑为他们带来了更多工作量，尤其是这些“额外的工作”与原有的业务逻辑耦合在一起，让系统越来越复杂，也越来越容易出错。</p>
</li>
<li><p><strong>第二阶段：将代码中的通信功能抽离重构成公共组件库，通信的可靠性由专业的平台程序员来保障</strong>。<br>开发人员解耦依赖的一贯有效办法是抽取分离代码与封装重构组件。微服务的普及离不开一系列封装了分布式通信能力的公共组件库，代表性产品有 Twitter 的 Finagle、Spring Cloud 中的许多组件等。这些公共的通信组件由熟悉分布式的专业开发人员编写和维护，不仅效率更高、质量更好，一般还都提供了经过良好设计的 API 接口，让业务代码既可以使用它们的能力，又无需把处理通信的逻辑散布于业务代码当中。</p>
<p><img src="https://raw.githubusercontent.com/wherezy1/PicGOImageShack/main/images/image-20230611230516301.png?token=AUR75H3DUQ26IVG4IDVU3XDEQXROS" srcset="/img/loading.gif" lazyload alt="image-20230611230516301">图 15-2 抽取公共的分布式通信组件</p>
<p>分布式通信组件让普通程序员开发出靠谱的微服务系统成为可能，这是无可抹杀的成绩，但普通程序员使用它们的成本依然很高，不仅要学习分布式的知识、要学习这些公共组件的功能应该如何使用，最麻烦的是，对于同一种问题往往还需学习多种不同的组件才能解决。这是因为通信组件首先是一段特定编程语言开发出来的程序，是与语言绑定的，一个由 Python 编写的组件再优秀，对 Java 系统来说也没有太多的实用价值。目前，基于公共组件库开发微服务仍然是应用最为广泛的解决方案，但肯定不是一种完美的解决方案，这是微服务基础设施完全成熟之前必然会出现的应用形态，同时也一定是微服务进化过程中必然会被替代的过渡形态。</p>
</li>
<li><p><strong>第三阶段：将负责通信的公共组件库分离到进程之外，程序间通过网络代理来交互，通信的可靠性由专门的网络代理提供商来保障</strong>。<br>为了能够把分布式通信组件与具体的编程语言脱钩，也为了避免程序员还要去专门学习这些组件的编程模型与 API 接口，这一阶段进化出了能专门负责可靠通信的网络代理。这些网络代理不再与业务逻辑部署于同一个进程空间，但仍然与业务系统处于同一个容器或者虚拟机当中，可以通过回环设备甚至是<a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Unix_domain_socket">UDS (opens new window)</a>（Unix Domain Socket）进行交互，具备相当高的网络性能。只要让网络代理接管掉程序七层或四层流量，就能够在代理上完成断路、容错等几乎所有的分布式通信功能，前面提到过的 Netflix Prana 就属于这类产品的典型代表。</p>
<p><img src="https://jingyecn.top:18080/assets/img/service-mesh-3.b272ad56.png" srcset="/img/loading.gif" lazyload alt="img"></p>
<p>图 15-3 通过网络代理获得可靠的通信能力</p>
<p>通过网络代理来提升通信质量的思路提出以后，它本身使用范围其实并不算特别广泛，但它的方向是正确的。这种思路后来演化出了两种改进形态：一方面，如果将网络代理从进程身边拉远，让它与进程分别处于不同的机器上，这样就可以同时给多个进程提供可靠通信的代理服务，这条路线逐渐形成了今天常见的微服务网关，在网关上同样可以实现流控、容错等功能。另一方面，如果将网络代理往进程方向推近，不仅让它与进程处于一个共享了网络名称空间的容器组之中，还要让它透明并强制地接管通讯，这便形成了下一阶段所说的边车代理。</p>
</li>
<li><p><strong>第四阶段：将网络代理以边车的形式注入到应用容器，自动劫持应用的网络流量，通信的可靠性由专门的通信基础设施来保障</strong>。<br>与前一阶段的独立代理相比，以边车模式运作的网络代理拥有两个无可比拟的优势：第一个优势是它对流量的劫持是强制性的，通常是靠直接写容器的 iptables 转发表来实现。此前，独立的网络代理只有程序首先去访问它，它才能被动地为程序提供可靠通信服务，只要程序依然有选择不访问它的可能性，代理就永远只能充当服务者而不能成为管理者，上阶段的图中保留了两个容器网络设备直接连接的箭头就代表这种可能性，而这一阶段图中，服务与网络名称空间的虚线箭头代表被劫持后应用程序以为存在，但实际并不存在的流量。<br>另一个优势是边车代理对应用是透明的，无需对已部署的应用程序代码进行任何改动，不需要引入任何的库（这点并不是绝对的，有部分边车代理也会要求有轻量级的 SDK），也不需要程序专门去访问某个特定的网络位置。这意味着它对所有现存程序都具备开箱即用的适应性，无需修改旧程序就能直接享受到边车代理的服务，这样它的适用面就变得十分广泛。目前边车代理的代表性产品有 Linkerd、Envoy、MOSN 等。</p>
<p><img src="https://jingyecn.top:18080/assets/img/service-mesh-4.ad21d5d1.png" srcset="/img/loading.gif" lazyload alt="img"></p>
<p>图 15-4 边车代理模式</p>
<p>如果说边车代理还有什么不足之处的话，那大概就是来自于运维人员的不满了。边车代理能够透明且具有强制力地解决可靠通信的问题，但它本身也需要有足够的信息才能完成这项工作，譬如获取可用服务的列表，譬如得到每个服务名称对应的 IP 地址，等等。这些信息不会从天上掉下来自动到边车里去，是需要由管理员主动去告知代理，或者代理主动从约定好的位置获取。可见，管理代理本身也会产生额外的通信需求。如果没有额外的支持，这些管理方面的通信都得由运维人员去埋单，由此而生的不满便可以理解。为了管理与协调边车代理，程序间通信进化到了最后一个阶段：服务网格。</p>
</li>
<li><p><strong>第五阶段：将边车代理统一管控起来实现安全、可控、可观测的通信，将数据平面与控制平面分离开来，实现通用、透明的通信，这项工作就由专门的服务网格框架来保障</strong>。<br>从总体架构看，服务网格包括两大块内容，分别是由一系列与微服务共同部署的边车代理，以及用于控制这些代理的管理器所构成。代理与代理之间需要通信，用以转发程序间通信的数据包；代理与管理器之间也需要通信，用以传递路由管理、服务发现、数据遥测等控制信息。服务网格使用<a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Forwarding_plane">数据平面 (opens new window)</a>（Data Plane）通信和<a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Control_plane">控制平面 (opens new window)</a>（Control Plane）通信来形容这两类流量，下图中实线就表示数据平面通信，虚线表示控制平面通信。</p>
<p><img src="https://jingyecn.top:18080/assets/img/service-mesh-5.71d1ba42.png" srcset="/img/loading.gif" lazyload alt="img"></p>
<p>图 15-5 服务网格的控制平面通信与数据平面通信（<a target="_blank" rel="noopener" href="https://philcalcado.com/2017/08/03/pattern_service_mesh.html">图片来源 (opens new window)</a>)</p>
<p>数据平面与控制平面并不是什么新鲜概念，它最初就是用在计算机网络之中的术语，通常是指网络层次的划分，软件定义网络中将解耦数据平面与控制平面作为其最主要特征之一。服务网格把计算机网络的经典概念引入到程序通信之中，既可以说是对程序通信的一种变革创新，也可以说是对网络通信的一种发展传承。<br>分离数据平面与控制平面的实质是将“程序”与“网络”进行解耦，将网络可能出现的问题（譬如中断后重试、降级），与可能需要的功能（譬如实现追踪度量）的处理过程从程序中拿出，放到由控制平面指导的数据平面通信中去处理，制造出一种“这些问题在程序间通信中根本不存在”的假象，仿佛网络和远程服务都是完美可靠的。这种完美的假象，让应用之间可以非常简单地交互而不必过多考虑异常情况，也能够在不同的程序框架、不同的云服务提供商环境之间平稳地迁移；同时，还能让管理者能够不依赖程序支持就得到遥测所需的全部信息，能够根据角色、权限进行统一的访问控制，这些都是服务网格的价值所在。</p>
</li>
</ul>
<h2 id="数据平面"><a href="#数据平面" class="headerlink" title="数据平面"></a>数据平面</h2><p>在“数据平面”和“控制平面”这两节里，笔者会延续服务网格将“程序”与“网络”解耦的思路，介绍几个数据平面通信与控制平面通信中的核心问题是如何解决的。在工业界，数据平面已有 Linkerd、Nginx、Envoy 等产品，控制平面也有 Istio、Open Service Mesh、Consul 等产品，后文中笔者主要是以目前市场占有率最高的 Istio 与 Envoy 为目标进行讲述，但讲述的目的是介绍两种平面通信的技术原理，而不是介绍 Istio 和 Envoy 的功能与用法，这里涉及到的原理在各种服务网格产品中一般都是通用的，并不局限于哪一种具体实现。</p>
<p>数据平面由一系列边车代理所构成，核心职责是转发应用的入站（Inbound）和出站（Outbound）数据包，因此数据平面也有个别名叫<a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Forwarding_plane">转发平面 (opens new window)</a>（Forwarding Plane）。同时，为了在不可靠的物理网络中保证程序间通信最大的可靠性，数据平面必须根据控制平面下发策略的指导，在应用无感知的情况下自动完成服务路由、健康检查、负载均衡、认证鉴权、产生监控数据等一系列工作。为了达成上述的工作目标，至少需要妥善解决以下三个关键问题：</p>
<ul>
<li>代理注入：边车代理是如何注入到应用程序中的？</li>
<li>流量劫持：边车代理是如何劫持应用程序的通信流量的？</li>
<li>可靠通信：边车代理是如何保证应用程序的通信可靠性的？</li>
</ul>
<h3 id="代理注入"><a href="#代理注入" class="headerlink" title="代理注入"></a>代理注入</h3><h3 id="流量劫持"><a href="#流量劫持" class="headerlink" title="流量劫持"></a>流量劫持</h3><h3 id="可靠通行"><a href="#可靠通行" class="headerlink" title="可靠通行"></a>可靠通行</h3><h2 id="控制平面"><a href="#控制平面" class="headerlink" title="控制平面"></a>控制平面</h2><p>如果说数据平面是行驶中的车辆，那控制平面就是车辆上的导航系统；如果说数据平面是城市的交通道路，那控制平面就是路口的指示牌与交通信号灯。控制平面的特点是不直接参与程序间通信，而只会与数据平面中的代理通信，在程序不可见的背后，默默地完成下发配置和策略，指导数据平面工作。由于服务网格（暂时）没有大规模引入计算机网络中<a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Management_plane">管理平面 (opens new window)</a>（Management Plane）等其他概念，所以控制平面通常也会附带地实现诸如网络行为的可视化、配置传输等一系列管理职能（其实还是有专门的管理平面工具的，譬如<a target="_blank" rel="noopener" href="https://github.com/layer5io/meshery">Meshery (opens new window)</a>、<a target="_blank" rel="noopener" href="https://github.com/solo-io/gloo-mesh">ServiceMeshHub (opens new window)</a>）。笔者仍然以 Istio 为例具体介绍一下控制平面的主要功能。</p>
<p>Istio 在 1.5 版本之前，Istio 自身也是采用微服务架构开发的，将控制平面的职责分解为 Mixer、Pilot、Galley、Citadel 四个模块去实现，其中 Mixer 负责鉴权策略与遥测；Pilot 负责对接 Envoy 的数据平面，遵循 xDS 协议进行策略分发；Galley 负责配置管理，为服务网格提供外部配置感知能力；Citadel 负责安全加密，提供服务和用户层面的认证和鉴权、管理凭据和 RBAC 等安全相关能力。不过，经过两、三年的实践应用，很多用户都有反馈 Istio 的微服务架构有过度设计的嫌疑，lstio 在定义项目目标时，曾非常理想化的提出控制平面的各个组件都应可以独立部署，然而在实际应用场景里却并非如此，独立的组件反而带来了部署复杂、职责划分不清晰等问题。</p>
<p><img src="https://jingyecn.top:18080/assets/img/istio-arch.9a97cf23.png" srcset="/img/loading.gif" lazyload alt="img"></p>
<p>图 15-9 Istio 1.5 版本之后的架构（图片来自<a target="_blank" rel="noopener" href="https://istio.io/latest/docs/ops/deployment/architecture/">Istio 官方文档 (opens new window)</a>)</p>
<p>从 1.5 版本起，Istio 重新回归单体架构，将 Pilot、Galley、Citadel 的功能全部集成到新的 Istiod 之中。当然，这也并不是说完全推翻之前的设计，只是将原有的多进程形态优化成单进程的形态，之前各个独立组件变成了 Istiod 的内部逻辑上的子模块而已。单体化之后出现的新进程 Istiod 就承担所有的控制平面职责，具体包括有：</p>
<ul>
<li><p>数据平面交互</p>
<p>：这是部分是满足服务网格正常工作所需的必要工作，具体有：</p>
<ul>
<li><strong>边车注入</strong>：在 Kubernetes 中注册 Mutating Webhook 控制器，实现代理容器的自动注入，并生成 Envoy 的启动配置信息。</li>
<li><strong>策略分发</strong>：接手了原来 Pilot 的核心工作，为所有的 Envoy 代理提供符合 xDS 协议的策略分发的服务。</li>
<li><strong>配置分发</strong>：接手了原来 Galley 的核心工作，负责监听来自多种支持配置源的数据，譬如 kube-apiserver，本地配置文件，或者定义为<a target="_blank" rel="noopener" href="https://github.com/istio/api/tree/master/mcp">网格配置协议 (opens new window)</a>（Mesh Configuration Protocol，MCP）的配置信息。原来 Galley 需要处理的 API 校验和配置转发功能也包含在内。</li>
</ul>
</li>
<li><p>流量控制</p>
<p>：这通常是用户使用服务网格的最主要目的，具体包括以下几个方面：</p>
<ul>
<li><strong>请求路由</strong>：通过<a target="_blank" rel="noopener" href="https://istio.io/latest/docs/reference/config/networking/virtual-service/">VirtualService (opens new window)</a>、<a target="_blank" rel="noopener" href="https://istio.io/latest/docs/reference/config/networking/destination-rule/">DestinationRule (opens new window)</a>等 Kubernetes CRD 资源实现了灵活的服务版本切分与规则路由。譬如根据服务的迭代版本号（如 v1.0 版、v2.0 版）、根据部署环境（如 Development 版、Production 版）作为路由规则来控制流量，实现诸如金丝雀发布这类应用需求。</li>
<li><strong>流量治理</strong>：包括熔断、超时、重试等功能，譬如通过修改 Envoy 的最大连接数，实现对请求的流量控制；通过修改负载均衡策略，在轮询、随机、最少访问等方式间进行切换；通过设置异常探测策略，将满足异常条件的实例从负载均衡池中摘除，以保证服务的稳定性，等等。</li>
<li><strong>调试能力</strong>：包括故障注入和流量镜像等功能，譬如在系统中人为的设置一些故障，来测试系统的容错稳定性和系统恢复的能力。又譬如通过复制一份请求流量，把它发送到镜像服务，从而满足<a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/A/B_testing">A&#x2F;B 验证 (opens new window)</a>的需要。</li>
</ul>
</li>
<li><p>通信安全</p>
<p>：包括通信中的加密、凭证、认证、授权等功能，具体有：</p>
<ul>
<li><strong>生成 CA 证书</strong>：接手了原来 Galley 的核心工作，负责生成通信加密所需私钥和 CA 证书。</li>
<li><strong>SDS 服务代理</strong>：最初 Istio 是通过 Kubernetes 的 Secret 卷的方式将证书分发到 Pod 中的，从 Istio 1.1 之后改为通过 SDS 服务代理来解决，这种方式保证了私钥证书不会在网络中传输，仅存在于 SDS 代理和 Envoy 的内存中，证书刷新轮换也不需要重启 Envoy。</li>
<li><strong>认证</strong>：提供基于节点的服务认证和基于请求的用户认证，这项功能笔者曾在服务安全的“<a target="_blank" rel="noopener" href="https://jingyecn.top:18080/distribution/secure/service-security.html#%E8%AE%A4%E8%AF%81">认证</a>”中详细介绍过。</li>
<li><strong>授权</strong>：提供不同级别的访问控制，这项功能笔者也曾在服务安全的“<a target="_blank" rel="noopener" href="https://jingyecn.top:18080/distribution/secure/service-security.html#%E6%8E%88%E6%9D%83">授权</a>”中详细介绍过。</li>
</ul>
</li>
<li><p>可观测性</p>
<p>：包括日志、追踪、度量三大块能力，具体有：</p>
<ul>
<li><strong>日志收集</strong>：程序日志的收集并不属于服务网格的处理范畴，通常会使用 ELK Stack 去完成，这里是指远程服务的访问日志的收集，对等的类比目标应该是以前 Nginx、Tomcat 的访问日志。</li>
<li><strong>链路追踪</strong>：为请求途经的所有服务生成分布式追踪数据并自动上报，运维人员可以通过 Zipkin 等追踪系统从数据中重建服务调用链，开发人员可以借此了解网格内服务的依赖和调用流程。</li>
<li><strong>指标度量</strong>：基于四类不同的监控标识（响应延迟、流量大小、错误数量、饱和度）生成一系列观测不同服务的监控指标，用于记录和展示网格中服务状态。****</li>
</ul>
</li>
</ul>

                
              </div>
            
            <hr/>
            <div>
              <div class="post-metas my-3">
  
    <div class="post-meta mr-3 d-flex align-items-center">
      <i class="iconfont icon-category"></i>
      

<span class="category-chains">
  
  
    
      <span class="category-chain">
        
  <a href="/categories/%E4%B9%A6%E7%B1%8D/" class="category-chain-item">书籍</a>
  
  

      </span>
    
  
</span>

    </div>
  
  
    <div class="post-meta">
      <i class="iconfont icon-tags"></i>
      
        <a href="/tags/%E4%B9%A6%E7%B1%8D/">#书籍</a>
      
        <a href="/tags/%E7%AC%94%E8%AE%B0/">#笔记</a>
      
    </div>
  
</div>


              
  

  <div class="license-box my-3">
    <div class="license-title">
      <div>凤凰架构</div>
      <div>http://example.com/2023/06/01/书籍-学习/凤凰架构/</div>
    </div>
    <div class="license-meta">
      
        <div class="license-meta-item">
          <div>作者</div>
          <div>where</div>
        </div>
      
      
        <div class="license-meta-item license-meta-date">
          <div>发布于</div>
          <div>2023年6月1日</div>
        </div>
      
      
      
        <div class="license-meta-item">
          <div>许可协议</div>
          <div>
            
              
              
                <a target="_blank" href="https://creativecommons.org/licenses/by/4.0/">
                  <span class="hint--top hint--rounded" aria-label="BY - 署名">
                    <i class="iconfont icon-by"></i>
                  </span>
                </a>
              
            
          </div>
        </div>
      
    </div>
    <div class="license-icon iconfont"></div>
  </div>



              
                <div class="post-prevnext my-3">
                  <article class="post-prev col-6">
                    
                    
                      <a href="/2023/06/01/Java%E7%B2%BE%E9%80%9A/JUC/" title="JUC">
                        <i class="iconfont icon-arrowleft"></i>
                        <span class="hidden-mobile">JUC</span>
                        <span class="visible-mobile">上一篇</span>
                      </a>
                    
                  </article>
                  <article class="post-next col-6">
                    
                    
                      <a href="/2023/06/01/ECS/k8s/k8s-%E9%BB%91%E9%A9%AC/" title="K8S-黑马">
                        <span class="hidden-mobile">K8S-黑马</span>
                        <span class="visible-mobile">下一篇</span>
                        <i class="iconfont icon-arrowright"></i>
                      </a>
                    
                  </article>
                </div>
              
            </div>

            
          </article>
        </div>
      </div>
    </div>

    <div class="side-col d-none d-lg-block col-lg-2">
      
  <aside class="sidebar" style="margin-left: -1rem">
    <div id="toc">
  <p class="toc-header">
    <i class="iconfont icon-list"></i>
    <span>目录</span>
  </p>
  <div class="toc-body" id="toc-body"></div>
</div>



  </aside>


    </div>
  </div>
</div>





  



  



  



  



  







    

    
      <a id="scroll-top-button" aria-label="TOP" href="#" role="button">
        <i class="iconfont icon-arrowup" aria-hidden="true"></i>
      </a>
    

    
      <div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel"
     aria-hidden="true">
  <div class="modal-dialog modal-dialog-scrollable modal-lg" role="document">
    <div class="modal-content">
      <div class="modal-header text-center">
        <h4 class="modal-title w-100 font-weight-bold">搜索</h4>
        <button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body mx-3">
        <div class="md-form mb-5">
          <input type="text" id="local-search-input" class="form-control validate">
          <label data-error="x" data-success="v" for="local-search-input">关键词</label>
        </div>
        <div class="list-group" id="local-search-result"></div>
      </div>
    </div>
  </div>
</div>

    

    
  </main>

  <footer>
    <div class="footer-inner">
  
    <div class="footer-content">
       <a href="https://hexo.io" target="_blank" rel="nofollow noopener"><span>Hexo</span></a> <i class="iconfont icon-love"></i> <a href="https://github.com/fluid-dev/hexo-theme-fluid" target="_blank" rel="nofollow noopener"><span>Fluid</span></a> 
    </div>
  
  
  
  
</div>

  </footer>

  <!-- Scripts -->
  
  <script  src="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.js" ></script>
  <link  rel="stylesheet" href="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.css" />

  <script>
    NProgress.configure({"showSpinner":false,"trickleSpeed":100})
    NProgress.start()
    window.addEventListener('load', function() {
      NProgress.done();
    })
  </script>


<script  src="https://lib.baomitu.com/jquery/3.6.0/jquery.min.js" ></script>
<script  src="https://lib.baomitu.com/twitter-bootstrap/4.6.1/js/bootstrap.min.js" ></script>
<script  src="/js/events.js" ></script>
<script  src="/js/plugins.js" ></script>


  <script  src="https://lib.baomitu.com/typed.js/2.0.12/typed.min.js" ></script>
  <script>
    (function (window, document) {
      var typing = Fluid.plugins.typing;
      var subtitle = document.getElementById('subtitle');
      if (!subtitle || !typing) {
        return;
      }
      var text = subtitle.getAttribute('data-typed-text');
      
        typing(text);
      
    })(window, document);
  </script>




  
    <script  src="/js/img-lazyload.js" ></script>
  




  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/tocbot/4.18.2/tocbot.min.js', function() {
    var toc = jQuery('#toc');
    if (toc.length === 0 || !window.tocbot) { return; }
    var boardCtn = jQuery('#board-ctn');
    var boardTop = boardCtn.offset().top;

    window.tocbot.init(Object.assign({
      tocSelector     : '#toc-body',
      contentSelector : '.markdown-body',
      linkClass       : 'tocbot-link',
      activeLinkClass : 'tocbot-active-link',
      listClass       : 'tocbot-list',
      isCollapsedClass: 'tocbot-is-collapsed',
      collapsibleClass: 'tocbot-is-collapsible',
      scrollSmooth    : true,
      includeTitleTags: true,
      headingsOffset  : -boardTop,
    }, CONFIG.toc));
    if (toc.find('.toc-list-item').length > 0) {
      toc.css('visibility', 'visible');
    }

    Fluid.events.registerRefreshCallback(function() {
      if ('tocbot' in window) {
        tocbot.refresh();
        var toc = jQuery('#toc');
        if (toc.length === 0 || !tocbot) {
          return;
        }
        if (toc.find('.toc-list-item').length > 0) {
          toc.css('visibility', 'visible');
        }
      }
    });
  });
</script>


  <script src=https://lib.baomitu.com/clipboard.js/2.0.11/clipboard.min.js></script>

  <script>Fluid.plugins.codeWidget();</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/anchor-js/4.3.1/anchor.min.js', function() {
    window.anchors.options = {
      placement: CONFIG.anchorjs.placement,
      visible  : CONFIG.anchorjs.visible
    };
    if (CONFIG.anchorjs.icon) {
      window.anchors.options.icon = CONFIG.anchorjs.icon;
    }
    var el = (CONFIG.anchorjs.element || 'h1,h2,h3,h4,h5,h6').split(',');
    var res = [];
    for (var item of el) {
      res.push('.markdown-body > ' + item.trim());
    }
    if (CONFIG.anchorjs.placement === 'left') {
      window.anchors.options.class = 'anchorjs-link-left';
    }
    window.anchors.add(res.join(', '));

    Fluid.events.registerRefreshCallback(function() {
      if ('anchors' in window) {
        anchors.removeAll();
        var el = (CONFIG.anchorjs.element || 'h1,h2,h3,h4,h5,h6').split(',');
        var res = [];
        for (var item of el) {
          res.push('.markdown-body > ' + item.trim());
        }
        if (CONFIG.anchorjs.placement === 'left') {
          anchors.options.class = 'anchorjs-link-left';
        }
        anchors.add(res.join(', '));
      }
    });
  });
</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.js', function() {
    Fluid.plugins.fancyBox();
  });
</script>


  <script>Fluid.plugins.imageCaption();</script>

  <script  src="/js/local-search.js" ></script>





<!-- 主题的启动项，将它保持在最底部 -->
<!-- the boot of the theme, keep it at the bottom -->
<script  src="/js/boot.js" ></script>


  

  <noscript>
    <div class="noscript-warning">博客在允许 JavaScript 运行的环境下浏览效果更佳</div>
  </noscript>
</body>
</html>
